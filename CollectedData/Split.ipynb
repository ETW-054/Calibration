{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10a92978-52d9-4cc8-b309-2a8874549ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c01a44-c290-4f08-8df4-97acc58764bf",
   "metadata": {},
   "source": [
    "# 解壓縮資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f34a0f5a-04f9-4bfb-9154-2001904235c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./front_pocket/202302071628/target.zip\n",
      "./front_pocket/202302071628/source.zip\n",
      "./front_pocket/202302071652/target.zip\n",
      "./front_pocket/202302071652/source.zip\n",
      "./front_pocket/202302071523/target.zip\n",
      "./front_pocket/202302071523/source.zip\n",
      "./front_pocket/202302071531/target.zip\n",
      "./front_pocket/202302071531/source.zip\n",
      "./front_pocket/202302071715/target.zip\n",
      "./front_pocket/202302071715/source.zip\n",
      "./front_pocket/202302071641/target.zip\n",
      "./front_pocket/202302071641/source.zip\n",
      "./front_pocket/202302071541/target.zip\n",
      "./front_pocket/202302071541/source.zip\n",
      "./front_pocket/202302071619/target.zip\n",
      "./front_pocket/202302071619/source.zip\n",
      "./front_pocket/202302071704/target.zip\n",
      "./front_pocket/202302071704/source.zip\n",
      "./front_pocket/202302071724/target.zip\n",
      "./front_pocket/202302071724/source.zip\n"
     ]
    }
   ],
   "source": [
    "for folder, _, files in os.walk('./front_pocket/'):\n",
    "    for file in files:\n",
    "        if file.endswith('zip'):\n",
    "            file_path = os.path.join(folder, file)\n",
    "            print(file_path)\n",
    "\n",
    "            sotre_path = os.path.join(folder, file.rsplit('.')[0])\n",
    "            # 開啟 ZIP 壓縮檔 \n",
    "            with zipfile.ZipFile(file_path, 'r') as zf:\n",
    "                # 解壓縮所有檔案至 /my/folder 目錄\n",
    "                zf.extractall(path=sotre_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d0f88-db27-4381-bfa6-8a7f86930454",
   "metadata": {},
   "source": [
    "# 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7b5f5c14-bd7c-4fb4-99c0-0a0cb769baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_data(df):\n",
    "    new_names = ['acc_times', 'acc_x', 'acc_y', 'acc_z', 'gyo_times', 'gyo_x', 'gyo_y', 'gyo_z', 'lin_acc_times', 'lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'mag_times', 'mag_x', 'mag_y', 'mag_z']\n",
    "    df.columns = new_names\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_original_data(path):\n",
    "    acc_df = pd.read_csv(os.path.join(path, 'Accelerometer.csv'), delimiter=';')\n",
    "    gyo_df = pd.read_csv(os.path.join(path, 'Gyroscope.csv'), delimiter=';')\n",
    "    linacc_df = pd.read_csv(os.path.join(path, 'Linear Accelerometer.csv'), delimiter=';')\n",
    "    mag_df = pd.read_csv(os.path.join(path, 'Magnetometer.csv'), delimiter=';')\n",
    "    \n",
    "    total_df = pd.concat([acc_df, gyo_df, linacc_df, mag_df], axis=1)\n",
    "    total_df = rename_data(total_df)\n",
    "    \n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "50962f15-f38a-429a-b00a-2adf9a6eb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound_range(df, second=10):\n",
    "    start = 20 * second\n",
    "    end = len(df) - 20 * second\n",
    "    \n",
    "    return df.iloc[start:end]\n",
    "\n",
    "\n",
    "def split_segments(df, second=5):\n",
    "    length = 20 * second\n",
    "    len_of_segs = int(np.floor(len(df) / length))\n",
    "    \n",
    "    segments = []\n",
    "    for i in range(len_of_segs):\n",
    "        segments.append(df.iloc[int(i * length):int((i + 1) * length)])\n",
    "        \n",
    "    return segments\n",
    "\n",
    "\n",
    "def select_data(df):\n",
    "    return df[['acc_times', 'acc_x', 'acc_y', 'acc_z', 'gyo_x', 'gyo_y', 'gyo_z', 'lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'mag_x', 'mag_y', 'mag_z']]\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    pre_df = bound_range(df)\n",
    "    pre_df = select_data(pre_df)\n",
    "    segs = split_segments(pre_df)\n",
    "    \n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99a24c-4c63-4e78-ae26-446adf71d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = load_original_data('./front_pocket/202302071724/source')\n",
    "# segs = preprocess_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "34aa6ce4-9bb1-428a-b421-0d086fc4cba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value    15.4\n",
       "Name: deviceRelease, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_device('./front_pocket/202302071523/source').loc['deviceRelease']  # source version: 15.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "096e5ad0-4606-4ef4-aa30-ff27a1d21cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value    16.3\n",
       "Name: deviceRelease, dtype: object"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_device('./front_pocket/202302071523/target').loc['deviceRelease']  # target version: 16.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c57b7b75-a898-49d3-8b69-a7ac8fe74114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_version(path):\n",
    "    device_df =  pd.read_csv(os.path.join(path, 'meta/device.csv'), delimiter=';', index_col=0)\n",
    "    return device_df.loc['deviceRelease'].value\n",
    "\n",
    "def load_pair_data(root_folder):\n",
    "    pair_data = []\n",
    "\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        source_path = os.path.join(folder_path, 'source')\n",
    "        target_path = os.path.join(folder_path, 'target')\n",
    "\n",
    "        #########################\n",
    "        ##### check devices #####\n",
    "        #########################\n",
    "        while True:\n",
    "            source_version = device_version(source_path)\n",
    "            target_version = device_version(target_path)\n",
    "            \n",
    "            print(source_path, target_path)\n",
    "            \n",
    "            if source_version == '15.4' and target_version == '16.3':\n",
    "                break\n",
    "            elif source_version == '16.3' and target_version == '15.4':\n",
    "                source_path = os.path.join(folder_path, 'target')\n",
    "                target_path = os.path.join(folder_path, 'source')\n",
    "                print('--- GG ---')\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        ####################################\n",
    "        ##### load and preprocess data #####\n",
    "        ####################################\n",
    "        \n",
    "        source_segs = preprocess_data(load_original_data(source_path))\n",
    "        target_segs = preprocess_data(load_original_data(target_path))\n",
    "\n",
    "        idx = min(len(source_segs), len(target_segs))\n",
    "\n",
    "        pair_data.extend(zip(source_segs[:idx], target_segs[:idx]))\n",
    "        \n",
    "    return pair_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1ee569c5-fda8-44e6-be6b-3aff9efb2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./front_pocket/202302071628/source ./front_pocket/202302071628/target\n",
      "./front_pocket/202302071652/source ./front_pocket/202302071652/target\n",
      "./front_pocket/202302071523/source ./front_pocket/202302071523/target\n",
      "./front_pocket/202302071531/source ./front_pocket/202302071531/target\n",
      "./front_pocket/202302071715/source ./front_pocket/202302071715/target\n",
      "./front_pocket/202302071641/source ./front_pocket/202302071641/target\n",
      "./front_pocket/202302071541/source ./front_pocket/202302071541/target\n",
      "./front_pocket/202302071619/source ./front_pocket/202302071619/target\n",
      "./front_pocket/202302071704/source ./front_pocket/202302071704/target\n",
      "./front_pocket/202302071724/source ./front_pocket/202302071724/target\n"
     ]
    }
   ],
   "source": [
    "pair_data = load_pair_data('./front_pocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46515db-8a56-47e9-8bbe-8283de894d0c",
   "metadata": {},
   "source": [
    "# 建立dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "073af591-76e1-47ec-a306-252417490eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "44d14471-de4a-4918-90be-e07462c99c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, pair_data):\n",
    "        self.pair_data = pair_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pair_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada036cd-d67b-4d25-b935-0cdcde61f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, seq_len=100, classes=2):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Linear(12, 24),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(24, 32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=32, hidden_size=32, num_layers=2, batch_first=True, bidirectional=True)\n",
    "#         self.lstm = nn.LSTM(input_size=16, hidden_size=16, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.last = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, classes),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.layer0(x)\n",
    "        \n",
    "        hz, _ = self.lstm(h)\n",
    "        \n",
    "        out = self.last(hz)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9366367b-10db-4dd5-972b-eb89ad061ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pair_data[:-50]\n",
    "valid_data = pair_data[-50:]\n",
    "\n",
    "# train\n",
    "train_dataset = PairDataset(\n",
    "                    pair_data=torch.tensor(np.array(pair_data), dtype=torch.float),\n",
    "                )\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# valid\n",
    "valid_dataset = PairDataset(\n",
    "                    pair_data=torch.tensor(np.array(pair_data), dtype=torch.float),\n",
    "                )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b88b083b-cc71-47d3-8491-780712b263eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674672d-763d-47dc-a44c-7e7820893068",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e23ee4-8da1-4fc7-bbd9-0b2397de5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "bce_loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba50865-9a85-43cb-8719-61f8868194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for sources, source_labels, targets in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sources = sources.to(device)\n",
    "        source_labels = source_labels.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        #############\n",
    "        # generator #\n",
    "        #############\n",
    "        predict_classes = model(sources)\n",
    "\n",
    "        loss = bce_loss()\n",
    "\n",
    "        # backward\n",
    "\n",
    "    ### mkae sequence save\n",
    "    if epoch > Tepoch:\n",
    "        seq_pos = seq_pos.detach().cpu().numpy().reshape(-1, seq_len, 2)\n",
    "    else:\n",
    "        seq_pos = np.zeros((len(target), seq_len, 2))\n",
    "\n",
    "    train_loss = np.mean(train_losses, axis=0, dtype=np.float64)\n",
    "    train_mag_loss = np.mean(train_mag_losses, axis=0, dtype=np.float64)\n",
    "\n",
    "    print(f'time: {time.time() - ep_start_time:>6.0f}s, ep: {epoch + 1:>5}, train: ms loss: {train_loss[3]:>5.3f}, generator loss: {train_loss[0]:>5.3f}, dis* loss: {train_loss[1]:>5.3f}, lstm dis* loss: {train_loss[4]:>5.3f}, seq dis* loss: {train_loss[5]:>5.3f}, pos loss: {train_loss[6]:>5.3f}' + \n",
    "          f'\\n{\" \":>32} mag: {np.mean(train_mag_loss[:3]):>5.3f} ({train_loss[2]:>5.3f}), magN: {train_mag_loss[0]:>5.3f}, magE: {train_mag_loss[1]:>5.3f}, magD: {train_mag_loss[2]:>5.3f}, magT: {train_mag_loss[3]:>5.3f}')\n",
    "    \n",
    "    return train_loss, train_mag_loss, seq_pos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mag39",
   "language": "python",
   "name": "mag39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
