{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1021aa84-1fd4-441f-980f-3dc168842784",
   "metadata": {},
   "source": [
    "```python\n",
    "file1 \\_ |-----------------------------------|   -+-\n",
    "file2 \\_ |-----------------------------------|    |   training data\n",
    "file3 \\_ |-----------------------------------|    |        80%\n",
    "file4 \\_ |-----------------------------------|    |\n",
    "                                                 -+-\n",
    "file5 \\_ |-----------------------------------|    |  validation data\n",
    "file6 \\_ |-----------------------------------|    |        20%\n",
    "         |-----------------------------------|   -+-\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a92978-52d9-4cc8-b309-2a8874549ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c01a44-c290-4f08-8df4-97acc58764bf",
   "metadata": {},
   "source": [
    "# 解壓縮資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34a0f5a-04f9-4bfb-9154-2001904235c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_data(path):\n",
    "    for folder, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('zip'):\n",
    "                file_path = os.path.join(folder, file)\n",
    "                print(file_path)\n",
    "\n",
    "                sotre_path = os.path.join(folder, file.rsplit('.')[0])\n",
    "                # 開啟 ZIP 壓縮檔 \n",
    "                with zipfile.ZipFile(file_path, 'r') as zf:\n",
    "                    # 解壓縮所有檔案至 /my/folder 目錄\n",
    "                    zf.extractall(path=sotre_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cecf48e8-21b8-46cd-a24d-6d79e9857780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip_data('./swing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986b396b-062a-4ebb-b6a1-1de47dbbb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv(path):\n",
    "    acc_df = pd.read_csv(os.path.join(path, 'Accelerometer.csv'), delimiter=',')\n",
    "    gyo_df = pd.read_csv(os.path.join(path, 'Gyroscope.csv'), delimiter=',')\n",
    "    linacc_df = pd.read_csv(os.path.join(path, 'Linear Accelerometer.csv'), delimiter=',')\n",
    "    mag_df = pd.read_csv(os.path.join(path, 'Magnetometer.csv'), delimiter=',')\n",
    "    device_df = pd.read_csv(os.path.join(path, 'meta', 'device.csv'), delimiter=',')\n",
    "    time_df = pd.read_csv(os.path.join(path, 'meta', 'time.csv'), delimiter=',')\n",
    "    \n",
    "    acc_df.to_csv(os.path.join(path, 'Accelerometer.csv'), index=False, sep=';')\n",
    "    gyo_df.to_csv(os.path.join(path, 'Gyroscope.csv'), index=False, sep=';')\n",
    "    linacc_df.to_csv(os.path.join(path, 'Linear Accelerometer.csv'), index=False, sep=';')\n",
    "    mag_df.to_csv(os.path.join(path, 'Magnetometer.csv'), index=False, sep=';')\n",
    "    device_df.to_csv(os.path.join(path, 'meta', 'device.csv'), index=False, sep=';')\n",
    "    time_df.to_csv(os.path.join(path, 'meta', 'time.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565c0b5b-15f7-4f3c-b0dd-4a8b67f34183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_csv('./pocket/202301101952/target')\n",
    "# convert_csv('./pocket/202301101952/source')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d0f88-db27-4381-bfa6-8a7f86930454",
   "metadata": {},
   "source": [
    "# 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b5f5c14-bd7c-4fb4-99c0-0a0cb769baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_data(df):\n",
    "    new_names = ['system_time', 'acc_times', 'acc_x', 'acc_y', 'acc_z', 'gyo_times', 'gyo_x', 'gyo_y', 'gyo_z', 'lin_acc_times', 'lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'mag_times', 'mag_x', 'mag_y', 'mag_z']\n",
    "    df.columns = new_names\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def device_start_system_time(path):\n",
    "    time_df = pd.read_csv(path, delimiter=';', index_col=0)\n",
    "    time = time_df.T.loc['system time', 'START']\n",
    "    \n",
    "    return time\n",
    "\n",
    "\n",
    "def load_original_data(path):\n",
    "    acc_df = pd.read_csv(os.path.join(path, 'Accelerometer.csv'), delimiter=';')\n",
    "    gyo_df = pd.read_csv(os.path.join(path, 'Gyroscope.csv'), delimiter=';')\n",
    "    linacc_df = pd.read_csv(os.path.join(path, 'Linear Accelerometer.csv'), delimiter=';')\n",
    "    mag_df = pd.read_csv(os.path.join(path, 'Magnetometer.csv'), delimiter=';')\n",
    "    start_time = device_start_system_time(os.path.join(path, 'meta/time.csv'))\n",
    "    time_df = acc_df.iloc[:, 0] + start_time\n",
    "    \n",
    "    total_df = pd.concat([time_df, acc_df, gyo_df, linacc_df, mag_df], axis=1)\n",
    "    total_df = rename_data(total_df)\n",
    "    \n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50962f15-f38a-429a-b00a-2adf9a6eb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_data(source_df, target_df):\n",
    "    source_start_time = source_df.loc[0, 'system_time']\n",
    "    target_start_time = target_df.loc[0, 'system_time']\n",
    "    \n",
    "    # align start time\n",
    "    if source_start_time > target_start_time:  # source start time > target start time\n",
    "        target_start_idx = np.argmin(np.abs(target_df.system_time - source_start_time))\n",
    "        target_df = target_df.iloc[target_start_idx:].reset_index(drop=True)\n",
    "    else:  # source start time < target start time\n",
    "        source_start_idx = np.argmin(np.abs(source_df.system_time - target_start_time))\n",
    "        source_df = source_df.iloc[source_start_idx:].reset_index(drop=True)\n",
    "        \n",
    "    # align end idx\n",
    "    end_idx = min(len(source_df), len(target_df))\n",
    "    source_df = source_df.iloc[:end_idx]\n",
    "    target_df = target_df.iloc[:end_idx]\n",
    "    \n",
    "    return source_df, target_df\n",
    "\n",
    "\n",
    "def bound_range(df):\n",
    "    start = datapoint_per_second * 35\n",
    "    end = len(df) - datapoint_per_second * 20\n",
    "    \n",
    "    return df.iloc[start:end].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def split_segments(df, length=150):\n",
    "    num_of_segs = int(np.floor(len(df) / length))\n",
    "    \n",
    "    segments = []\n",
    "    for i in range(num_of_segs):\n",
    "        seg = df.iloc[int(i * length):int((i + 1) * length)].to_numpy()\n",
    "        segments.append(seg)\n",
    "        \n",
    "    return segments\n",
    "\n",
    "def split_segments_overlap(df, length=150):\n",
    "    num_of_chunk = int(np.floor(len(df) / length)) * 2 + 1\n",
    "    \n",
    "    segments = []\n",
    "    for i in np.arange(0, num_of_chunk, 0.5):\n",
    "        seg = df.iloc[int(i * length):int((i + 1) * length)].to_numpy()\n",
    "        segments.append(seg)\n",
    "        \n",
    "    return segments\n",
    "\n",
    "# def split_segments_overlap(df, chunk_size=5, seq_len=25):\n",
    "#     total_length = chunk_size * seq_len\n",
    "    \n",
    "#     segments = []\n",
    "#     for i in range(0, len(df) - total_length, 5):\n",
    "#         seg = df.iloc[i:i + total_length].to_numpy()\n",
    "#         segments.append(np.array(np.split(seg, chunk_size)))\n",
    "        \n",
    "#     return segments\n",
    "\n",
    "\n",
    "def select_data(df):\n",
    "    return df[['lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'gyo_x', 'gyo_y', 'gyo_z', 'mag_x', 'mag_y', 'mag_z', 'system_time']]\n",
    "\n",
    "\n",
    "def preprocess_data(df, length=150, is_train_data=False):\n",
    "    pre_df = select_data(df)\n",
    "    \n",
    "    if is_train_data:\n",
    "        segs = split_segments_overlap(pre_df, length)\n",
    "    else:\n",
    "        segs = split_segments(pre_df, length)\n",
    "    \n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c57b7b75-a898-49d3-8b69-a7ac8fe74114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_version(path):\n",
    "    device_df = pd.read_csv(path, delimiter=';', index_col=0)\n",
    "    version = device_df.loc['deviceRelease'].value\n",
    "    \n",
    "    return version\n",
    "\n",
    "\n",
    "def check_data_device(source_path, target_path):\n",
    "    while True:\n",
    "        source_version = device_version(os.path.join(source_path, 'meta/device.csv'))\n",
    "        target_version = device_version(os.path.join(target_path, 'meta/device.csv'))\n",
    "\n",
    "        print(source_path, target_path)\n",
    "\n",
    "        if source_version[:2] == '15' and target_version[:2] == '16':\n",
    "            return source_path, target_path\n",
    "        elif source_version[:2] == '16' and target_version[:2] == '15':\n",
    "            source_path = os.path.join(folder_path, 'target')\n",
    "            target_path = os.path.join(folder_path, 'source')\n",
    "            print('--- GG ---')\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def load_pair_data(root_folder, class_num, file_index=None, is_train_data=False):\n",
    "    pair_data = []\n",
    "    files = os.listdir(root_folder)\n",
    "    if file_index is not None:\n",
    "        files = files[file_index[0]:file_index[1]]\n",
    "\n",
    "    for folder in files:\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        source_path = os.path.join(folder_path, 'source')\n",
    "        target_path = os.path.join(folder_path, 'target')\n",
    "        \n",
    "        print(folder_path)\n",
    "        \n",
    "        #########################\n",
    "        ##### check devices #####\n",
    "        #########################\n",
    "        source_path, target_path = check_data_device(source_path, target_path)\n",
    "        \n",
    "        ####################################\n",
    "        ##### load and preprocess data #####\n",
    "        ####################################\n",
    "        source_df = load_original_data(source_path)\n",
    "        target_df = load_original_data(target_path)\n",
    "        \n",
    "#         print(source_df.system_time[0], target_df.system_time[0])\n",
    "        \n",
    "        source_df, target_df = align_data(source_df, target_df)\n",
    "        source_df, target_df = bound_range(source_df), bound_range(target_df)\n",
    "        \n",
    "#         print(source_df.system_time[0], target_df.system_time[0])\n",
    "#         print(source_df.system_time[len(source_df) - 1], target_df.system_time[len(target_df) - 1])\n",
    "#         print(len(source_df), len(target_df))\n",
    "        \n",
    "#         plt.figure(figsize=(30, 5))\n",
    "#         plt.plot(np.arange(len(source_df)), source_df.acc_x)\n",
    "#         plt.plot(np.arange(len(target_df)), target_df.acc_y)\n",
    "#         plt.show()\n",
    "        \n",
    "        source_segs = preprocess_data(source_df, length, is_train_data)\n",
    "        target_segs = preprocess_data(target_df, length, is_train_data)\n",
    "        \n",
    "        idx = min(len(source_segs), len(target_segs))\n",
    "        source_tags = [class_num] * idx\n",
    "        target_tags = [0] * idx\n",
    "        \n",
    "        pair_data.extend(zip(source_segs[:idx], source_tags, target_segs[:idx], target_tags))\n",
    "        \n",
    "    return pair_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d299628-a975-4dc5-ba26-5ae000e5995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint_per_second = 20\n",
    "duration = 2\n",
    "chunk_size = 150\n",
    "seq_len = 1\n",
    "length = 150\n",
    "classes = {0: 'target', 1: 'front_pocket', 2: 'pocket', 3: 'swing'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34aa6ce4-9bb1-428a-b421-0d086fc4cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source device version: 15.4\n",
      "target device version: 16.3\n"
     ]
    }
   ],
   "source": [
    "source_device_version = device_version('./front_pocket/202302071523/source/meta/device.csv')\n",
    "target_device_version = device_version('./front_pocket/202302071523/target/meta/device.csv')\n",
    "print(f\"source device version: {source_device_version}\")  # source version: 15.4\n",
    "print(f\"target device version: {target_device_version}\")  # target version: 16.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ee569c5-fda8-44e6-be6b-3aff9efb2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./front_pocket/202302071628\n",
      "./front_pocket/202302071628/source ./front_pocket/202302071628/target\n",
      "./front_pocket/202302071652\n",
      "./front_pocket/202302071652/source ./front_pocket/202302071652/target\n",
      "./front_pocket/202302071523\n",
      "./front_pocket/202302071523/source ./front_pocket/202302071523/target\n",
      "./front_pocket/202302071531\n",
      "./front_pocket/202302071531/source ./front_pocket/202302071531/target\n",
      "./front_pocket/202302071715\n",
      "./front_pocket/202302071715/source ./front_pocket/202302071715/target\n",
      "./front_pocket/202302071641\n",
      "./front_pocket/202302071641/source ./front_pocket/202302071641/target\n",
      "./front_pocket/202302071541\n",
      "./front_pocket/202302071541/source ./front_pocket/202302071541/target\n",
      "./front_pocket/202302071619\n",
      "./front_pocket/202302071619/source ./front_pocket/202302071619/target\n",
      "./front_pocket/202302071704\n",
      "./front_pocket/202302071704/source ./front_pocket/202302071704/target\n",
      "./pocket/202302132108\n",
      "./pocket/202302132108/source ./pocket/202302132108/target\n",
      "./pocket/202302131750\n",
      "./pocket/202302131750/source ./pocket/202302131750/target\n",
      "./pocket/202302131601\n",
      "./pocket/202302131601/source ./pocket/202302131601/target\n",
      "./pocket/202302071606\n",
      "./pocket/202302071606/source ./pocket/202302071606/target\n",
      "./pocket/202302132053\n",
      "./pocket/202302132053/source ./pocket/202302132053/target\n",
      "./pocket/202302122132\n",
      "./pocket/202302122132/source ./pocket/202302122132/target\n",
      "./pocket/202302132116\n",
      "./pocket/202302132116/source ./pocket/202302132116/target\n",
      "./pocket/202302131643\n",
      "./pocket/202302131643/source ./pocket/202302131643/target\n",
      "./pocket/202301101952\n",
      "./pocket/202301101952/source ./pocket/202301101952/target\n",
      "./swing/202302142339\n",
      "./swing/202302142339/source ./swing/202302142339/target\n",
      "./swing/202302132131\n",
      "./swing/202302132131/source ./swing/202302132131/target\n",
      "./swing/202302142117\n",
      "./swing/202302142117/source ./swing/202302142117/target\n",
      "./swing/202302132124\n",
      "./swing/202302132124/source ./swing/202302132124/target\n",
      "./swing/202302121947\n",
      "./swing/202302121947/source ./swing/202302121947/target\n",
      "./swing/202302121857\n",
      "./swing/202302121857/source ./swing/202302121857/target\n",
      "./swing/202302142128\n",
      "./swing/202302142128/source ./swing/202302142128/target\n",
      "./swing/202302121920\n",
      "./swing/202302121920/source ./swing/202302121920/target\n",
      "./swing/202302121909\n",
      "./swing/202302121909/source ./swing/202302121909/target\n"
     ]
    }
   ],
   "source": [
    "train_front_pocket_pair_data = load_pair_data('./front_pocket', class_num=1, file_index=[0, 8], is_train_data=True)\n",
    "valid_front_pocket_pair_data = load_pair_data('./front_pocket', class_num=1, file_index=[8, 10], is_train_data=False)\n",
    "\n",
    "train_pocket_pair_data = load_pair_data('./pocket', class_num=2, file_index=[0, 8], is_train_data=True)\n",
    "valid_pocket_pair_data = load_pair_data('./pocket', class_num=2, file_index=[8, 10], is_train_data=False)\n",
    "\n",
    "train_swing_pair_data = load_pair_data('./swing', class_num=3, file_index=[0, 8], is_train_data=True)\n",
    "valid_swing_pair_data = load_pair_data('./swing', class_num=3, file_index=[8, 10], is_train_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73a5c707-8a25-4a06-874d-df286b5599c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082 1062 1026\n"
     ]
    }
   ],
   "source": [
    "print(len(train_front_pocket_pair_data), len(train_pocket_pair_data), len(train_swing_pair_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d2f40e0-0448-490c-9449-232c6a55bd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-9.85151611e-01,  2.26427228e+00,  1.02551626e+00, -1.13609666e-02,\n",
       "        -1.03316814e-01, -3.52177948e-01,  1.71319580e+01, -1.81716766e+01,\n",
       "         3.65813904e+01,  1.67575811e+09]),\n",
       " array([[ 1.50048383e+00,  6.38481451e-01, -5.94772929e-01, ...,\n",
       "          2.39695740e+00,  3.12113648e+01,  1.67575812e+09],\n",
       "        [ 3.64921688e+00,  8.30549621e-01, -1.06992710e+00, ...,\n",
       "          2.80191040e+00,  3.17483521e+01,  1.67575812e+09],\n",
       "        [ 1.14059092e+00, -1.19653445e+00, -6.04431360e-01, ...,\n",
       "          2.04116821e+00,  3.33367920e+01,  1.67575812e+09],\n",
       "        ...,\n",
       "        [-4.80639080e-01, -2.40715367e+00,  1.35310300e+00, ...,\n",
       "         -2.12767487e+01,  4.46415405e+01,  1.67575812e+09],\n",
       "        [ 4.58150104e-01, -4.34717095e-01,  2.14802028e+00, ...,\n",
       "         -1.93531418e+01,  4.66666870e+01,  1.67575813e+09],\n",
       "        [-1.01109220e+00,  2.41413408e-01,  2.46635439e+00, ...,\n",
       "         -1.80263061e+01,  4.86743774e+01,  1.67575813e+09]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_front_pocket_pair_data[0][0][0], train_front_pocket_pair_data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d821c35e-fd9d-4cc0-8149-54acca32aefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, tuple, numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_front_pocket_pair_data), type(train_front_pocket_pair_data[1]), type(train_front_pocket_pair_data[0][0]), type(train_front_pocket_pair_data[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46515db-8a56-47e9-8bbe-8283de894d0c",
   "metadata": {},
   "source": [
    "# 建立dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "073af591-76e1-47ec-a306-252417490eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6048176-d0ec-4f39-aa20-afd7b53758da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, source_data, source_label, target_data, target_label):\n",
    "        self.source_data = source_data\n",
    "        self.source_label = source_label\n",
    "        self.target_data = target_data\n",
    "        self.target_label = target_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_data[idx], self.source_label[idx], self.target_data[idx], self.target_label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9f8d6ad-89c6-40f3-b9d5-e7357b0d8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstDerivativeLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean', chunk_size=5):\n",
    "        super(FirstDerivativeLoss, self).__init__()\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        # calculate the first derivative\n",
    "        source_o = torch.reshape(source, (len(source), self.chunk_size, -1))\n",
    "        target_o = torch.reshape(target, (len(target), self.chunk_size, -1))\n",
    "        d_source = source_o[1:] - source_o[:-1]\n",
    "        d_target = target_o[1:] - target_o[:-1]\n",
    "        deriv = d_source - d_target\n",
    "\n",
    "        # calculate the loss as the mean squared error of the derivative\n",
    "        loss = torch.mean(torch.pow(deriv, 2))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30960412-1af6-47ed-90d8-0c8a7dc4a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(z_imu, target_imu, predict_mag, target_mag):\n",
    "    imu_loss = mse_loss(z_imu, target_imu)\n",
    "    mag_loss = mse_loss(predict_mag, target_mag)\n",
    "    d1_loss = div_loss(predict_mag, target_mag)\n",
    "\n",
    "    total_loss = imu_loss + mag_loss #+ d1_loss\n",
    "    return total_loss, imu_loss, mag_loss, d1_loss\n",
    "\n",
    "# def discriminator_loss(d_real, d_fake):\n",
    "#     real_loss = bce_loss(d_real, torch.ones_like(d_real))\n",
    "#     fake_loss = bce_loss(d_fake, torch.zeros_like(d_fake))\n",
    "\n",
    "#     return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9366367b-10db-4dd5-972b-eb89ad061ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2686798/2038108984.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_source_data = np.array([d[0] for d in train_data])\n",
      "/tmp/ipykernel_2686798/2038108984.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_target_data = np.array([d[2] for d in train_data])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m train_target_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([d[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m train_data])\n\u001b[1;32m      8\u001b[0m train_target_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([d[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m train_data])\n\u001b[1;32m      9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m PairDataset(\n\u001b[0;32m---> 10\u001b[0m                     source_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_source_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     11\u001b[0m                     source_label \u001b[38;5;241m=\u001b[39m train_source_label,\n\u001b[1;32m     12\u001b[0m                     target_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_target_data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[1;32m     13\u001b[0m                     target_label \u001b[38;5;241m=\u001b[39m train_target_label,\n\u001b[1;32m     14\u001b[0m                 )\n\u001b[1;32m     15\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# valid\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "train_data = train_front_pocket_pair_data + train_pocket_pair_data + train_swing_pair_data\n",
    "valid_data = valid_front_pocket_pair_data + valid_pocket_pair_data + valid_swing_pair_data\n",
    "\n",
    "# train\n",
    "train_source_data = np.array([d[0] for d in train_data])\n",
    "train_source_label = np.array([d[1] for d in train_data])\n",
    "train_target_data = np.array([d[2] for d in train_data])\n",
    "train_target_label = np.array([d[3] for d in train_data])\n",
    "train_dataset = PairDataset(\n",
    "                    source_data = torch.tensor(train_source_data, dtype=torch.float),\n",
    "                    source_label = train_source_label,\n",
    "                    target_data = torch.tensor(train_target_data, dtype=torch.float),\n",
    "                    target_label = train_target_label,\n",
    "                )\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# valid\n",
    "valid_source_data = np.array([d[0] for d in valid_data])\n",
    "valid_source_label = np.array([d[1] for d in valid_data])\n",
    "valid_target_data = np.array([d[2] for d in valid_data])\n",
    "valid_target_label = np.array([d[3] for d in valid_data])\n",
    "valid_dataset = PairDataset(\n",
    "                    source_data = torch.tensor(valid_source_data, dtype=torch.float),\n",
    "                    source_label = valid_source_label,\n",
    "                    target_data = torch.tensor(valid_target_data, dtype=torch.float),\n",
    "                    target_label = valid_target_label,\n",
    "                )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88b083b-cc71-47d3-8491-780712b263eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4733b36-7de2-49e6-ac7f-a58f9e9917ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tgt_mask(size) -> torch.tensor:\n",
    "    # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "    mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "    mask = mask.float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "    mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "\n",
    "    # EX for size=5:\n",
    "    # [[0., -inf, -inf, -inf, -inf],\n",
    "    #  [0.,   0., -inf, -inf, -inf],\n",
    "    #  [0.,   0.,   0., -inf, -inf],\n",
    "    #  [0.,   0.,   0.,   0., -inf],\n",
    "    #  [0.,   0.,   0.,   0.,   0.]]\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ada036cd-d67b-4d25-b935-0cdcde61f750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, chunk_size=5, seq_len=21, num_of_classes=2):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_of_classes = num_of_classes\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Linear(6, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        # 16 * (seq_len-8)\n",
    "        self.rnn = nn.RNN(input_size=16 * seq_len, hidden_size=64, num_layers=2, batch_first=True)\n",
    "        \n",
    "        self.last = nn.Sequential(\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, num_of_classes),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):  # input: (bs, chunk_size, seq_len, 9)\n",
    "        h = torch.reshape(x, (len(x) * self.chunk_size, self.seq_len, -1))  # (bs, chunk_size, seq_len, 9) -> (bs * chunk_size, seq_len, 9)\n",
    "        \n",
    "        h = self.cnn(h)\n",
    "\n",
    "        h = torch.reshape(h, (len(x), self.chunk_size, -1))\n",
    "        \n",
    "        hz, _ = self.rnn(h)\n",
    "        out = self.last(hz)\n",
    "\n",
    "        out = torch.reshape(out, (len(x), self.chunk_size, self.num_of_classes))\n",
    "        \n",
    "        return out, hz\n",
    "    \n",
    "    def predict_label(self, x):\n",
    "        predict_probability, _ = self(x)\n",
    "        predict_probability = predict_probability.reshape((-1, self.num_of_classes))\n",
    "        _, predict_classes = torch.max(predict_probability, 1)\n",
    "        one_hot = F.one_hot(predict_classes, num_classes=self.num_of_classes).to(device).float()\n",
    "        class_result = one_hot.reshape(-1, self.chunk_size, self.num_of_classes)\n",
    "        \n",
    "        return class_result\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :].to(x.device)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class NotSimpleTransformer(nn.Module):\n",
    "    def __init__(self, chunk_size=5, seq_len=21, num_of_classes=2):\n",
    "        super(NotSimpleTransformer, self).__init__()\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_of_classes = num_of_classes\n",
    "        \n",
    "        self.imu_layer = nn.Sequential(\n",
    "            nn.Linear(6, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.mag_layer = nn.Sequential(\n",
    "            nn.Linear(3, 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.pos_encoder = PositionalEncoding(64 * self.seq_len, 0.1)\n",
    "        self.mag_pos_encoder = PositionalEncoding(16 * self.seq_len, 0.1)\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=64 * self.seq_len, nhead=8, dropout=0.2, batch_first=True), num_layers=4)\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(64, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 6),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.task_controller_net = nn.Sequential(\n",
    "            nn.Linear(1348, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(64, 9),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model=64* self.seq_len, nhead=8, dropout=0.1, batch_first=True), num_layers=4)\n",
    "        \n",
    "#         self.mag_last = nn.Sequential(\n",
    "#             nn.Linear(64, 16),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(16, 3),\n",
    "#         )\n",
    "\n",
    "        self.mag_last = nn.Sequential(\n",
    "            nn.Linear(16, 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, 3),\n",
    "        )\n",
    "        \n",
    "    def forward(self, source_imu, source_mag, class_result):\n",
    "        \n",
    "        ### step 1\n",
    "        h_imu = self.imu_layer(source_imu)\n",
    "        h_imu = h_imu.reshape((len(source_imu), self.chunk_size, -1))\n",
    "        h_imu = self.pos_encoder(h_imu)\n",
    "        \n",
    "        h_mag = source_mag\n",
    "#         h_mag = h_mag.reshape((len(source_mag), self.chunk_size, -1))\n",
    "#         h_mag = self.mag_pos_encoder(h_mag)\n",
    "        \n",
    "        ### step 2\n",
    "        z = self.encoder(h_imu)\n",
    "        z_imu = z.reshape((len(source_imu), self.chunk_size, self.seq_len, -1))\n",
    "        z_imu = self.nn(z_imu)\n",
    "        \n",
    "        z_class = torch.concat([z, class_result], dim=-1)\n",
    "        task_latent = self.task_controller_net(z_class)\n",
    "        \n",
    "        ### step 3\n",
    "#         pred_latent = self.decoder(h_mag, z)\n",
    "#         pred_mag = pred_latent.reshape((len(source_mag), self.chunk_size, self.seq_len, -1))\n",
    "        \n",
    "#         pred_mag = pred_mag + task_latent[:, :, None, :]\n",
    "#         pred_mag = self.mag_last(pred_mag)\n",
    "\n",
    "        task_matrix = task_latent.reshape((len(source_mag), self.chunk_size, 3, 3))  # (bs, cs, 3, 3)\n",
    "        #task_matrix = self.mag_last(task_matrix)\n",
    "        pred_mag = torch.matmul(h_mag, task_matrix)\n",
    "        \n",
    "#         pred_mag = self.mag_last(pred_mag)\n",
    "#         pred_mag = pred_mag + finetune_mag\n",
    "        \n",
    "        \n",
    "        return z_imu, pred_mag\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a9b697b-6b12-4dd2-a2bf-5d9242762705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, chunk_size=5, seq_len=21):\n",
    "#         super(Discriminator, self).__init__()\n",
    "        \n",
    "#         self.chunk_size = chunk_size\n",
    "#         self.seq_len = seq_len\n",
    "        \n",
    "#         self.feature_extractor = nn.Sequential(\n",
    "#             nn.Linear(3, 16),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(16, 32),\n",
    "#             nn.LeakyReLU(),\n",
    "#         )\n",
    "        \n",
    "#         self.lstm = nn.LSTM(input_size=32 * self.seq_len, hidden_size=32 * self.seq_len, num_layers=2, batch_first=True, bidirectional=True)\n",
    "#         self.lstm = nn.LSTM(input_size=32 * self.seq_len, hidden_size=32 * self.seq_len, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "#         self.discriminant_layer = nn.Sequential(\n",
    "#             nn.Linear(64 * self.seq_len, 16),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(16, 1),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         h = self.feature_extractor(x)  # (bs, chunk_size, seq_len, 3) -> (bs, chunk_size, seq_len, 16)\n",
    "        \n",
    "#         h = h.reshape((len(x), self.chunk_size, -1))  # (bs, chunk_size, seq_len, 16) -> (bs, chunk_size, seq_len * 16)\n",
    "#         hz, _ = self.lstm(h)  # (bs, chunk_size, seq_len * 16) -> (bs, chunk_size, seq_len * 16 * 2)\n",
    "        \n",
    "#         out = self.discriminant_layer(hz)\n",
    "        \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1674672d-763d-47dc-a44c-7e7820893068",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 2000\n",
    "num_of_classes = 4\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "bce_loss = torch.nn.BCELoss()\n",
    "div_loss = FirstDerivativeLoss(chunk_size=chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ecf49e-94c2-400e-9820-fa7650d12576",
   "metadata": {},
   "source": [
    "# Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31e23ee4-8da1-4fc7-bbd9-0b2397de5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(chunk_size=chunk_size, seq_len=seq_len, num_of_classes=num_of_classes).to(device)\n",
    "optimizer_C = torch.optim.Adam(classifier.parameters(), lr=0.00005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_C, T_max=50, eta_min=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d4a3006-be31-4aee-9378-13a1c756e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_C(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for source_data, source_label, target_data, target_label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sequences = torch.concat([source_data[:, :, :, :6], target_data[::3, :, :, :6]])\n",
    "        labels = torch.concat([source_label, target_label[::3]])\n",
    "        \n",
    "        sequences = sequences.to(device)\n",
    "        one_hot = F.one_hot(labels, num_classes=num_of_classes).to(device).float()\n",
    "        one_hot = one_hot[:, None, :].repeat((1, chunk_size, 1)).float()\n",
    "\n",
    "        #############\n",
    "        # generator #\n",
    "        #############\n",
    "        predict_probability, _ = model(sequences)\n",
    "        \n",
    "        predict_probability = predict_probability.reshape((-1, num_of_classes))\n",
    "        one_hot = one_hot.reshape((-1, num_of_classes))\n",
    "        loss = ce_loss(predict_probability, one_hot)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predict_classes = torch.max(predict_probability, 1)\n",
    "        predict_classes = predict_classes.cpu().detach().numpy()\n",
    "        _, labels = torch.max(one_hot, 1)\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(accuracy_score(labels, predict_classes))\n",
    "    \n",
    "    return np.mean(losses), np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fe3c5bb-2e6d-4e27-bd7b-5424da79d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_C(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for source_data, source_label, target_data, target_label in dataloader:\n",
    "            \n",
    "            sequences = torch.concat([source_data[:, :, :, :6], target_data[:, :, :, :6]])\n",
    "            labels = torch.concat([source_label, target_label])\n",
    "            \n",
    "            sequences = sequences.to(device)\n",
    "            one_hot = F.one_hot(labels, num_classes=num_of_classes).to(device).float()\n",
    "            one_hot = one_hot[:, None, :].repeat((1, chunk_size, 1)).float()\n",
    "\n",
    "            #############\n",
    "            # generator #\n",
    "            #############\n",
    "            predict_probability, _ = model(sequences[:, :, :, :9])\n",
    "            \n",
    "            predict_probability = predict_probability.reshape((-1, num_of_classes))\n",
    "            one_hot = one_hot.reshape((-1, num_of_classes))\n",
    "            loss = ce_loss(predict_probability, one_hot)\n",
    "\n",
    "            _, predict_classes = torch.max(predict_probability, 1)\n",
    "            predict_classes = predict_classes.cpu().detach().numpy()\n",
    "            _, labels = torch.max(one_hot, 1)\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(accuracy_score(labels, predict_classes))\n",
    "    \n",
    "    return np.mean(losses), np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d01fd93b-ff56-474b-8e56-e3820a1052c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000: train loss: 0.746, acc: 0.998   valid loss: 0.784, acc: 0.959\n",
      "00001: train loss: 0.746, acc: 0.998   valid loss: 0.791, acc: 0.952\n",
      "00002: train loss: 0.746, acc: 0.998   valid loss: 0.779, acc: 0.964\n",
      "00003: train loss: 0.746, acc: 0.998   valid loss: 0.776, acc: 0.967\n",
      "00004: train loss: 0.746, acc: 0.998   valid loss: 0.768, acc: 0.975\n",
      "00005: train loss: 0.746, acc: 0.998   valid loss: 0.780, acc: 0.963\n",
      "00006: train loss: 0.746, acc: 0.998   valid loss: 0.784, acc: 0.959\n",
      "00007: train loss: 0.746, acc: 0.998   valid loss: 0.780, acc: 0.963\n",
      "00008: train loss: 0.746, acc: 0.998   valid loss: 0.776, acc: 0.967\n",
      "00009: train loss: 0.746, acc: 0.998   valid loss: 0.762, acc: 0.981\n",
      "00010: train loss: 0.746, acc: 0.998   valid loss: 0.770, acc: 0.973\n",
      "00011: train loss: 0.746, acc: 0.998   valid loss: 0.768, acc: 0.975\n",
      "00012: train loss: 0.746, acc: 0.998   valid loss: 0.770, acc: 0.973\n",
      "00013: train loss: 0.746, acc: 0.998   valid loss: 0.774, acc: 0.970\n",
      "00014: train loss: 0.746, acc: 0.998   valid loss: 0.772, acc: 0.971\n",
      "00015: train loss: 0.746, acc: 0.998   valid loss: 0.774, acc: 0.969\n",
      "00016: train loss: 0.746, acc: 0.998   valid loss: 0.774, acc: 0.970\n",
      "00017: train loss: 0.746, acc: 0.998   valid loss: 0.785, acc: 0.958\n",
      "00018: train loss: 0.746, acc: 0.998   valid loss: 0.782, acc: 0.962\n",
      "00019: train loss: 0.746, acc: 0.998   valid loss: 0.775, acc: 0.968\n",
      "00020: train loss: 0.746, acc: 0.998   valid loss: 0.773, acc: 0.970\n",
      "00021: train loss: 0.746, acc: 0.998   valid loss: 0.777, acc: 0.966\n",
      "00022: train loss: 0.746, acc: 0.998   valid loss: 0.773, acc: 0.970\n",
      "00023: train loss: 0.746, acc: 0.998   valid loss: 0.768, acc: 0.975\n",
      "00024: train loss: 0.746, acc: 0.998   valid loss: 0.768, acc: 0.975\n",
      "00025: train loss: 0.746, acc: 0.998   valid loss: 0.775, acc: 0.968\n",
      "00026: train loss: 0.746, acc: 0.998   valid loss: 0.777, acc: 0.966\n",
      "00027: train loss: 0.746, acc: 0.998   valid loss: 0.777, acc: 0.966\n",
      "00028: train loss: 0.746, acc: 0.998   valid loss: 0.779, acc: 0.964\n",
      "00029: train loss: 0.746, acc: 0.998   valid loss: 0.767, acc: 0.976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_C\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_C\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      4\u001b[0m     valid_loss, valid_acc \u001b[38;5;241m=\u001b[39m evalute_C(classifier, valid_loader)\n",
      "Cell \u001b[0;32mIn[25], line 31\u001b[0m, in \u001b[0;36mtrain_C\u001b[0;34m(model, dataloader, optimizer)\u001b[0m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m _, predict_classes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(predict_probability, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m predict_classes \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_classes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     32\u001b[0m _, labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(one_hot, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    train_loss, train_acc = train_C(classifier, train_loader, optimizer_C)\n",
    "    lr_scheduler.step()\n",
    "    valid_loss, valid_acc = evalute_C(classifier, valid_loader)\n",
    "    \n",
    "    ep = str(epoch).zfill(5)\n",
    "\n",
    "    print(f'{ep}: train loss: {train_loss:2.3f}, acc: {train_acc:2.3f}   valid loss: {valid_loss:2.3f}, acc: {valid_acc:2.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967d8fd-b6dd-4aca-ba04-1f8ffbd9f834",
   "metadata": {},
   "source": [
    "# Train whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a8f5097-7203-43ff-9020-76d38adfd545",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NotSimpleTransformer(chunk_size=chunk_size, seq_len=seq_len, num_of_classes=num_of_classes).to(device)\n",
    "# discriminator = Discriminator(chunk_size=chunk_size, seq_len=seq_len).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "# optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.000025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fec0d49-bec9-4777-be88-ba2261557cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(result):\n",
    "    pred_mag = result[0].reshape(-1, 3)\n",
    "    targ_mag = result[1].reshape(-1, 3)\n",
    "    x = np.arange(len(pred_mag))\n",
    "    \n",
    "    loss = mse_loss(torch.tensor(pred_mag), torch.tensor(targ_mag)).item()\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for pm, tm, caxis, color in zip(pred_mag.T, targ_mag.T, ['x', 'y', 'z'], ['tab:blue', 'tab:orange', 'tab:green']):\n",
    "        plt.plot(x, pm, label=f'pred_{caxis}', color=color, linewidth=2.5, zorder=2)\n",
    "        plt.plot(x, tm, '--', label=f'targ_{caxis}', color=color, linewidth=1, alpha=0.8, zorder=1)\n",
    "    \n",
    "    for i in range(0, chunk_size+1):\n",
    "        plt.axvline(x=int(i * seq_len - 0.5), linewidth=0.5, linestyle='-', color='k', alpha=0.5)\n",
    "    \n",
    "    plt.suptitle(f'Loss: {loss:.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eba50865-9a85-43cb-8719-61f8868194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, draw=False):\n",
    "    model.train()\n",
    "    classifier.eval()\n",
    "\n",
    "    g_losses = []\n",
    "    imu_losses = []\n",
    "    mag_losses = []\n",
    "    d1_losses = []\n",
    "\n",
    "    for source_data, source_label, target_data, target_label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        source_data = source_data.to(device)\n",
    "        source_imu = source_data[:, :, :, :6]\n",
    "        source_mag = source_data[:, :, :, 6:9]\n",
    "        \n",
    "        target_data = target_data.to(device)\n",
    "        target_imu = target_data[:, :, :, :6]\n",
    "        target_mag = target_data[:, :, :, 6:9]\n",
    "\n",
    "        # generate mag\n",
    "        class_result = classifier.predict_label(source_imu)\n",
    "        \n",
    "        z_imu, predict_mag = model(source_imu, source_mag, class_result)\n",
    "        \n",
    "        # generator loss\n",
    "        gen_loss, imu_loss, mag_loss, d1_loss = generator_loss(z_imu, target_imu, predict_mag, target_mag)\n",
    "        \n",
    "        # backward\n",
    "        gen_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # record loss\n",
    "        g_losses.append(gen_loss.item())\n",
    "        imu_losses.append(imu_loss.item())\n",
    "        mag_losses.append(mag_loss.item())\n",
    "        d1_losses.append(d1_loss.item())\n",
    "    \n",
    "    if draw:\n",
    "        rs = [predict_mag[0].detach().cpu().numpy(), target_mag[0].cpu().numpy()]\n",
    "        plot_result(rs)\n",
    "    \n",
    "    return np.mean(g_losses), np.mean(imu_losses), np.mean(mag_losses), np.mean(d1_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cb16192-e16c-4d77-bc8f-7d1b6eff1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute(dataloader, draw=False):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    g_losses = []\n",
    "    imu_losses = []\n",
    "    mag_losses = []\n",
    "    d1_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for source_data, source_label, target_data, target_label in dataloader:\n",
    "            source_data = source_data.to(device)\n",
    "            source_imu = source_data[:, :, :, :6]\n",
    "            source_mag = source_data[:, :, :, 6:9]\n",
    "\n",
    "            target_data = target_data.to(device)\n",
    "            target_imu = target_data[:, :, :, :6]\n",
    "            target_mag = target_data[:, :, :, 6:9]\n",
    "\n",
    "            # generate mag\n",
    "            class_result = classifier.predict_label(source_imu)\n",
    "            \n",
    "            z_imu, predict_mag = model(source_imu, source_mag, class_result)\n",
    "\n",
    "            # generator loss\n",
    "            gen_loss, imu_loss, mag_loss, d1_loss = generator_loss(z_imu, target_imu, predict_mag, target_mag)\n",
    "\n",
    "            # record loss\n",
    "            g_losses.append(gen_loss.item())\n",
    "            imu_losses.append(imu_loss.item())\n",
    "            mag_losses.append(mag_loss.item())\n",
    "            d1_losses.append(d1_loss.item())\n",
    "    \n",
    "    if draw:\n",
    "        rs = [predict_mag[0].detach().cpu().numpy(), target_mag[0].cpu().numpy()]\n",
    "        plot_result(rs)\n",
    "    \n",
    "    return np.mean(g_losses), np.mean(imu_losses), np.mean(mag_losses), np.mean(d1_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8754fe8-3516-4cd9-8c5a-cf81084e2465",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Sequential: 1, Linear: 2, LeakyReLU: 2, Linear: 2, LeakyReLU: 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mag39/lib/python3.9/site-packages/torchinfo/torchinfo.py:288\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 288\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/mag39/lib/python3.9/site-packages/torch/nn/modules/module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1128\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[0;32mIn[21], line 128\u001b[0m, in \u001b[0;36mNotSimpleTransformer.forward\u001b[0;34m(self, source_imu, source_mag, class_result)\u001b[0m\n\u001b[1;32m    127\u001b[0m h_imu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimu_layer(source_imu)\n\u001b[0;32m--> 128\u001b[0m h_imu \u001b[38;5;241m=\u001b[39m \u001b[43mh_imu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource_imu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m h_imu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(h_imu)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 150, -1]' is invalid for input of size 9408",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m----> 2\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mag39/lib/python3.9/site-packages/torchinfo/torchinfo.py:218\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m validate_user_params(\n\u001b[1;32m    212\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    215\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    216\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    217\u001b[0m )\n\u001b[0;32m--> 218\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    222\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    223\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    224\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mag39/lib/python3.9/site-packages/torchinfo/torchinfo.py:297\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    296\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Sequential: 1, Linear: 2, LeakyReLU: 2, Linear: 2, LeakyReLU: 2]"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=[(1, 7, 21, 6), (1, 7, 21, 3), (1, 7, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e651a-66ef-44aa-951b-3ef632d6bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in classifier.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4fc917-c3da-4c6c-94b5-2fb7d8c4b11d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check():\n",
    "    for source_data, source_label, target_data, target_label in train_loader:\n",
    "        source_data = source_data.to(device)\n",
    "        source_imu = source_data[:, :, :, :6]\n",
    "        source_mag = source_data[:, :, :, 6:9]\n",
    "        \n",
    "        predict_probability, _ = classifier(source_imu)\n",
    "        predict_probability = predict_probability.reshape((-1, num_of_classes))\n",
    "        _, predict_classes = torch.max(predict_probability, 1)\n",
    "        one_hot = F.one_hot(predict_classes, num_classes=num_of_classes).to(device).float()\n",
    "        class_result = one_hot.reshape(-1, chunk_size, num_of_classes)\n",
    "        \n",
    "        ### step 1\n",
    "        h_imu = model.imu_layer(source_imu)\n",
    "        h_imu = h_imu.reshape((len(source_imu), chunk_size, -1))\n",
    "        h_imu = model.pos_encoder(h_imu)\n",
    "        \n",
    "        h_mag = model.mag_layer(source_mag)\n",
    "        h_mag = h_mag.reshape((len(source_mag), chunk_size, -1))\n",
    "        h_mag = model.pos_encoder(h_mag)\n",
    "        \n",
    "        ### step 2\n",
    "        z = model.encoder(h_imu)\n",
    "        z_imu = z.reshape((len(source_imu), chunk_size, seq_len, -1))\n",
    "        z_imu = model.nn(z_imu)\n",
    "\n",
    "        z_class = torch.concat([z, class_result], dim=-1)\n",
    "        task_latent = model.task_controller_net(z_class)\n",
    "        \n",
    "        ### step 3\n",
    "        pred_latent = model.decoder(h_mag, z)\n",
    "        pred_mag = pred_latent.reshape((len(source_mag), chunk_size, seq_len, -1))\n",
    "        \n",
    "        pred_mag = pred_mag + task_latent[:, :, None, :]\n",
    "        pred_mag = model.mag_last(pred_mag)\n",
    "        \n",
    "        \n",
    "        return z_imu, pred_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e74f03-1f6e-4245-976b-7ef42b7c366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = False\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        draw = True\n",
    "    \n",
    "    train_loss, train_imu_loss, train_mag_loss, t_d1_loss = train(train_loader, draw)\n",
    "    valid_loss, valid_imu_loss, valid_mag_loss, v_d1_loss = evalute(valid_loader, draw)\n",
    "    \n",
    "    draw = False\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        ep = str(epoch).zfill(5)\n",
    "        print(f'{ep:>5}: train loss: {train_loss: 2.3f}, imu loss: {train_imu_loss: 2.3f}, mag loss: {train_mag_loss: 2.3f}, div loss: {t_d1_loss: 2.3f}\\n' +\n",
    "              f'{\"\":>5}  valid loss: {valid_loss: 2.3f}, imu loss: {valid_imu_loss: 2.3f}, mag loss: {valid_mag_loss: 2.3f}, div loss: {v_d1_loss: 2.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d28d2b-c05c-4053-81ab-da651c91028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_eval(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    class_results = []\n",
    "    class_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for source_data, source_label, target_data, target_label in dataloader:\n",
    "            source_data = source_data.to(device)\n",
    "            source_imu = source_data[:, :, :, :6]\n",
    "            source_mag = source_data[:, :, :, 6:9]\n",
    "\n",
    "            target_data = target_data.to(device)\n",
    "            target_imu = target_data[:, :, :, :6]\n",
    "            target_mag = target_data[:, :, :, 6:9]\n",
    "            \n",
    "            class_result = classifier.predict_label(source_imu)\n",
    "\n",
    "            # generate mag\n",
    "            z_imu, predict_mag = model(source_imu, source_mag, class_result)\n",
    "            \n",
    "\n",
    "            # generator loss\n",
    "#             gen_loss, imu_loss, mag_loss = generator_loss(z_imu, target_imu, predict_mag, target_mag)\n",
    "\n",
    "            # record loss\n",
    "            predict_mag = predict_mag.detach().cpu().numpy()\n",
    "            target_mag = target_mag.cpu().numpy()\n",
    "            source_label = source_label.detach().cpu().numpy()\n",
    "            \n",
    "            results.extend(zip(predict_mag, target_mag, source_label))\n",
    "            \n",
    "            diff = np.power(predict_mag.reshape(len(predict_mag), -1, 3) - target_mag.reshape(len(target_mag), -1, 3), 2).mean(axis=1)\n",
    "            class_results.extend(diff)\n",
    "            class_labels.extend(source_label)\n",
    "    \n",
    "    return np.array(results, dtype=object), np.array(class_results), np.array(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939dee9b-ce00-4ab3-a471-02eeecec79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(result):\n",
    "    pred_mag = result[0].reshape(-1, 3)\n",
    "    targ_mag = result[1].reshape(-1, 3)\n",
    "    source_label = result[2]\n",
    "    x = np.arange(len(pred_mag))\n",
    "    \n",
    "    loss = mse_loss(torch.tensor(pred_mag), torch.tensor(targ_mag)).item()\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for pm, tm, caxis, color in zip(pred_mag.T, targ_mag.T, ['x', 'y', 'z'], ['tab:blue', 'tab:orange', 'tab:green']):\n",
    "        plt.plot(x, pm, label=f'pred_{caxis}', color=color, linewidth=2.5, zorder=2)\n",
    "        plt.plot(x, tm, '--', label=f'targ_{caxis}', color=color, linewidth=1, alpha=0.8, zorder=1)\n",
    "    \n",
    "    for i in range(0, chunk_size+1):\n",
    "        plt.axvline(x=int(i * seq_len - 0.5), linewidth=0.5, linestyle='-', color='k', alpha=0.5)\n",
    "    \n",
    "    plt.suptitle(f'{classes[source_label]}   Loss: {loss:.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c0beb-8280-4a77-a23a-e421405bb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, cres, clabel = output_eval(model, train_loader)\n",
    "print(cres.shape, clabel.shape)\n",
    "\n",
    "print(f'{\"loss name\":>20}| {\"diff x\":>8} {\"diff y\":>8} {\"diff z\":>8} {\"avg\":>8}')\n",
    "for class_label, class_name in classes.items():\n",
    "    if sum(clabel == class_label) <= 0:\n",
    "        continue\n",
    "    dx, dy, dz = cres[clabel == class_label].mean(axis=0)\n",
    "    print(f'{class_name:>15} loss: {dx:>8.2f} {dy:>8.2f} {dz:>8.2f} {np.mean([dx, dy, dz]):>8.2f}')\n",
    "dx, dy, dz = cres.mean(axis=0)\n",
    "print(f'{\"average\":>15} loss: {dx:>8.2f} {dy:>8.2f} {dz:>8.2f} {np.mean([dx, dy, dz]):>8.2f}')\n",
    "\n",
    "for rs in results[::5000]:\n",
    "    plot_result(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dca13b-1f6d-40dc-895d-8ce7403c954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, cres, clabel = output_eval(model, valid_loader)\n",
    "print(cres.shape, clabel.shape)\n",
    "\n",
    "print(f'{\"loss name\":>20}| {\"diff x\":>8} {\"diff y\":>8} {\"diff z\":>8} {\"avg\":>8}')\n",
    "for class_label, class_name in classes.items():\n",
    "    if sum(clabel == class_label) <= 0:\n",
    "        continue\n",
    "    dx, dy, dz = cres[clabel == class_label].mean(axis=0)\n",
    "    print(f'{class_name:>15} loss: {dx:>8.2f} {dy:>8.2f} {dz:>8.2f} {np.mean([dx, dy, dz]):>8.2f}')\n",
    "dx, dy, dz = cres.mean(axis=0)\n",
    "print(f'{\"average\":>15} loss: {dx:>8.2f} {dy:>8.2f} {dz:>8.2f} {np.mean([dx, dy, dz]):>8.2f}')\n",
    "\n",
    "for rs in results[::50]:\n",
    "    plot_result(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf58007-cc88-4b25-8844-b3fe898ad6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674c46f-c42f-4416-afc9-bf5484da583a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d7a77-c59f-4a38-9501-0f7c1c378d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mag39",
   "language": "python",
   "name": "mag39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
