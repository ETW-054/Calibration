{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78bacccb-2bf6-426f-9818-3d28ae40881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c01a44-c290-4f08-8df4-97acc58764bf",
   "metadata": {},
   "source": [
    "# 解壓縮資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34a0f5a-04f9-4bfb-9154-2001904235c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_data(path):\n",
    "    for folder, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('zip'):\n",
    "                file_path = os.path.join(folder, file)\n",
    "                print(file_path)\n",
    "\n",
    "                sotre_path = os.path.join(folder, file.rsplit('.')[0])\n",
    "                # 開啟 ZIP 壓縮檔 \n",
    "                with zipfile.ZipFile(file_path, 'r') as zf:\n",
    "                    # 解壓縮所有檔案至 /my/folder 目錄\n",
    "                    zf.extractall(path=sotre_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cecf48e8-21b8-46cd-a24d-6d79e9857780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip_data('./swing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986b396b-062a-4ebb-b6a1-1de47dbbb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv(path):\n",
    "    acc_df = pd.read_csv(os.path.join(path, 'Accelerometer.csv'), delimiter=',')\n",
    "    gyo_df = pd.read_csv(os.path.join(path, 'Gyroscope.csv'), delimiter=',')\n",
    "    linacc_df = pd.read_csv(os.path.join(path, 'Linear Accelerometer.csv'), delimiter=',')\n",
    "    mag_df = pd.read_csv(os.path.join(path, 'Magnetometer.csv'), delimiter=',')\n",
    "    device_df = pd.read_csv(os.path.join(path, 'meta', 'device.csv'), delimiter=',')\n",
    "    time_df = pd.read_csv(os.path.join(path, 'meta', 'time.csv'), delimiter=',')\n",
    "    \n",
    "    acc_df.to_csv(os.path.join(path, 'Accelerometer.csv'), index=False, sep=';')\n",
    "    gyo_df.to_csv(os.path.join(path, 'Gyroscope.csv'), index=False, sep=';')\n",
    "    linacc_df.to_csv(os.path.join(path, 'Linear Accelerometer.csv'), index=False, sep=';')\n",
    "    mag_df.to_csv(os.path.join(path, 'Magnetometer.csv'), index=False, sep=';')\n",
    "    device_df.to_csv(os.path.join(path, 'meta', 'device.csv'), index=False, sep=';')\n",
    "    time_df.to_csv(os.path.join(path, 'meta', 'time.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565c0b5b-15f7-4f3c-b0dd-4a8b67f34183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_csv('./pocket/202301101952/target')\n",
    "# convert_csv('./pocket/202301101952/source')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d0f88-db27-4381-bfa6-8a7f86930454",
   "metadata": {},
   "source": [
    "# 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b5f5c14-bd7c-4fb4-99c0-0a0cb769baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_data(df):\n",
    "    new_names = ['system_time', 'acc_times', 'acc_x', 'acc_y', 'acc_z', 'gyo_times', 'gyo_x', 'gyo_y', 'gyo_z', 'lin_acc_times', 'lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'mag_times', 'mag_x', 'mag_y', 'mag_z']\n",
    "    df.columns = new_names\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def device_start_system_time(path):\n",
    "    time_df = pd.read_csv(path, delimiter=';', index_col=0)\n",
    "    time = time_df.T.loc['system time', 'START']\n",
    "    \n",
    "    return time\n",
    "\n",
    "\n",
    "def load_original_data(path):\n",
    "    acc_df = pd.read_csv(os.path.join(path, 'Accelerometer.csv'), delimiter=';')\n",
    "    gyo_df = pd.read_csv(os.path.join(path, 'Gyroscope.csv'), delimiter=';')\n",
    "    linacc_df = pd.read_csv(os.path.join(path, 'Linear Accelerometer.csv'), delimiter=';')\n",
    "    mag_df = pd.read_csv(os.path.join(path, 'Magnetometer.csv'), delimiter=';')\n",
    "    start_time = device_start_system_time(os.path.join(path, 'meta/time.csv'))\n",
    "    time_df = acc_df.iloc[:, 0] + start_time\n",
    "    \n",
    "    total_df = pd.concat([time_df, acc_df, gyo_df, linacc_df, mag_df], axis=1)\n",
    "    total_df = rename_data(total_df)\n",
    "    \n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50962f15-f38a-429a-b00a-2adf9a6eb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_data(source_df, target_df):\n",
    "    source_start_time = source_df.loc[0, 'system_time']\n",
    "    target_start_time = target_df.loc[0, 'system_time']\n",
    "    \n",
    "    # align start time\n",
    "    if source_start_time > target_start_time:  # source start time > target start time\n",
    "        target_start_idx = np.argmin(np.abs(target_df.system_time - source_start_time))\n",
    "        target_df = target_df.iloc[target_start_idx:].reset_index(drop=True)\n",
    "    else:  # source start time < target start time\n",
    "        source_start_idx = np.argmin(np.abs(source_df.system_time - target_start_time))\n",
    "        source_df = source_df.iloc[source_start_idx:].reset_index(drop=True)\n",
    "        \n",
    "    # align end idx\n",
    "    end_idx = min(len(source_df), len(target_df))\n",
    "    source_df = source_df.iloc[:end_idx]\n",
    "    target_df = target_df.iloc[:end_idx]\n",
    "    \n",
    "    return source_df, target_df\n",
    "\n",
    "\n",
    "def bound_range(df):\n",
    "    start = datapoint_per_second * 35\n",
    "    end = len(df) - datapoint_per_second * 20\n",
    "    \n",
    "    return df.iloc[start:end].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def split_segments(df, chunk_size=5, seq_len=25):\n",
    "#     length = datapoint_per_second * duration\n",
    "    length = chunk_size * seq_len\n",
    "    num_of_segs = int(np.floor(len(df) / length))\n",
    "    \n",
    "    segments = []\n",
    "    for i in range(num_of_segs):\n",
    "        seg = df.iloc[int(i * length):int((i + 1) * length)].to_numpy()\n",
    "        segments.append(np.array(np.split(seg, chunk_size)))\n",
    "        \n",
    "    return segments\n",
    "\n",
    "\n",
    "def overlap_split(df, chunk_size, seq_len):\n",
    "    cks = chunk_size + 1\n",
    "    half_len = int(seq_len / 2)\n",
    "    length = cks * half_len\n",
    "    num_of_segs = int(np.floor(len(df) / length))\n",
    "    \n",
    "    segments = []\n",
    "    ck_id = np.zeros((chunk_size, 2), dtype=int)\n",
    "    ck_id[:, 0] = np.arange(chunk_size)\n",
    "    ck_id[:, 1] = ck_id[:, 0] + 1\n",
    "    print(ck_id)\n",
    "    \n",
    "    for i in range(num_of_segs):\n",
    "        seg = df.iloc[int(i * length):int((i + 1) * length)].to_numpy()\n",
    "        split_seg = np.array(np.split(seg, cks))\n",
    "\n",
    "        segments.append(split_seg[ck_id].reshape(chunk_size, seq_len, -1))\n",
    "        \n",
    "    return segments\n",
    "    \n",
    "\n",
    "def select_data(df):\n",
    "    return df[['acc_x', 'acc_y', 'acc_z', 'gyo_x', 'gyo_y', 'gyo_z', 'lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'mag_x', 'mag_y', 'mag_z', 'system_time']]\n",
    "\n",
    "\n",
    "def preprocess_data(df, chunk_size=5, seq_len=25):\n",
    "    pre_df = select_data(df)\n",
    "    segs = split_segments(pre_df, chunk_size, seq_len)\n",
    "    \n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c57b7b75-a898-49d3-8b69-a7ac8fe74114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_version(path):\n",
    "    device_df = pd.read_csv(path, delimiter=';', index_col=0)\n",
    "    version = device_df.loc['deviceRelease'].value\n",
    "    \n",
    "    return version\n",
    "\n",
    "\n",
    "def check_data_device(source_path, target_path):\n",
    "    while True:\n",
    "        source_version = device_version(os.path.join(source_path, 'meta/device.csv'))\n",
    "        target_version = device_version(os.path.join(target_path, 'meta/device.csv'))\n",
    "\n",
    "        print(source_path, target_path)\n",
    "\n",
    "        if source_version[:2] == '15' and target_version[:2] == '16':\n",
    "            return source_path, target_path\n",
    "        elif source_version[:2] == '16' and target_version[:2] == '15':\n",
    "            source_path = os.path.join(folder_path, 'target')\n",
    "            target_path = os.path.join(folder_path, 'source')\n",
    "            print('--- GG ---')\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def load_pair_data(root_folder, class_num):\n",
    "    pair_data = []\n",
    "\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        source_path = os.path.join(folder_path, 'source')\n",
    "        target_path = os.path.join(folder_path, 'target')\n",
    "        \n",
    "        print(folder_path)\n",
    "        \n",
    "        #########################\n",
    "        ##### check devices #####\n",
    "        #########################\n",
    "        source_path, target_path = check_data_device(source_path, target_path)\n",
    "        \n",
    "        ####################################\n",
    "        ##### load and preprocess data #####\n",
    "        ####################################\n",
    "        source_df = load_original_data(source_path)\n",
    "        target_df = load_original_data(target_path)\n",
    "        \n",
    "#         print(source_df.system_time[0], target_df.system_time[0])\n",
    "        \n",
    "        source_df, target_df = align_data(source_df, target_df)\n",
    "        source_df, target_df = bound_range(source_df), bound_range(target_df)\n",
    "        \n",
    "#         print(source_df.system_time[0], target_df.system_time[0])\n",
    "#         print(source_df.system_time[len(source_df) - 1], target_df.system_time[len(target_df) - 1])\n",
    "#         print(len(source_df), len(target_df))\n",
    "        \n",
    "#         plt.figure(figsize=(30, 5))\n",
    "#         plt.plot(np.arange(len(source_df)), source_df.acc_x)\n",
    "#         plt.plot(np.arange(len(target_df)), target_df.acc_y)\n",
    "#         plt.show()\n",
    "        \n",
    "        source_segs = preprocess_data(source_df, chunk_size, seq_len)\n",
    "        target_segs = preprocess_data(target_df, chunk_size, seq_len)\n",
    "        \n",
    "        idx = min(len(source_segs), len(target_segs))\n",
    "        source_tags = [class_num] * idx\n",
    "        target_tags = [0] * idx\n",
    "        \n",
    "        pair_data.extend(zip(source_segs[:idx], source_tags, target_segs[:idx], target_tags))\n",
    "        \n",
    "    return pair_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d299628-a975-4dc5-ba26-5ae000e5995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint_per_second = 20\n",
    "duration = 2\n",
    "chunk_size = 10\n",
    "seq_len = 24\n",
    "classes = {'target': 0, 'front_pocket': 1, 'pocket': 2, 'swing': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34aa6ce4-9bb1-428a-b421-0d086fc4cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source device version: 15.4\n",
      "target device version: 16.3\n"
     ]
    }
   ],
   "source": [
    "source_device_version = device_version('./front_pocket/202302071523/source/meta/device.csv')\n",
    "target_device_version = device_version('./front_pocket/202302071523/target/meta/device.csv')\n",
    "print(f\"source device version: {source_device_version}\")  # source version: 15.4\n",
    "print(f\"target device version: {target_device_version}\")  # target version: 16.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ee569c5-fda8-44e6-be6b-3aff9efb2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./front_pocket/202302071628\n",
      "./front_pocket/202302071628/source ./front_pocket/202302071628/target\n",
      "./front_pocket/202302071652\n",
      "./front_pocket/202302071652/source ./front_pocket/202302071652/target\n",
      "./front_pocket/202302071523\n",
      "./front_pocket/202302071523/source ./front_pocket/202302071523/target\n",
      "./front_pocket/202302071531\n",
      "./front_pocket/202302071531/source ./front_pocket/202302071531/target\n",
      "./front_pocket/202302071715\n",
      "./front_pocket/202302071715/source ./front_pocket/202302071715/target\n",
      "./front_pocket/202302071641\n",
      "./front_pocket/202302071641/source ./front_pocket/202302071641/target\n",
      "./front_pocket/202302071541\n",
      "./front_pocket/202302071541/source ./front_pocket/202302071541/target\n",
      "./front_pocket/202302071619\n",
      "./front_pocket/202302071619/source ./front_pocket/202302071619/target\n",
      "./front_pocket/202302071704\n",
      "./front_pocket/202302071704/source ./front_pocket/202302071704/target\n",
      "./front_pocket/202302071724\n",
      "./front_pocket/202302071724/source ./front_pocket/202302071724/target\n",
      "./pocket/202302132108\n",
      "./pocket/202302132108/source ./pocket/202302132108/target\n",
      "./pocket/202302131750\n",
      "./pocket/202302131750/source ./pocket/202302131750/target\n",
      "./pocket/202302131601\n",
      "./pocket/202302131601/source ./pocket/202302131601/target\n",
      "./pocket/202302071606\n",
      "./pocket/202302071606/source ./pocket/202302071606/target\n",
      "./pocket/202302132053\n",
      "./pocket/202302132053/source ./pocket/202302132053/target\n",
      "./pocket/202302122132\n",
      "./pocket/202302122132/source ./pocket/202302122132/target\n",
      "./pocket/202302132116\n",
      "./pocket/202302132116/source ./pocket/202302132116/target\n",
      "./pocket/202302131643\n",
      "./pocket/202302131643/source ./pocket/202302131643/target\n",
      "./pocket/202301101952\n",
      "./pocket/202301101952/source ./pocket/202301101952/target\n",
      "./pocket/202302132101\n",
      "./pocket/202302132101/source ./pocket/202302132101/target\n",
      "./swing/202302142339\n",
      "./swing/202302142339/source ./swing/202302142339/target\n",
      "./swing/202302132131\n",
      "./swing/202302132131/source ./swing/202302132131/target\n",
      "./swing/202302142117\n",
      "./swing/202302142117/source ./swing/202302142117/target\n",
      "./swing/202302132124\n",
      "./swing/202302132124/source ./swing/202302132124/target\n",
      "./swing/202302121947\n",
      "./swing/202302121947/source ./swing/202302121947/target\n",
      "./swing/202302121857\n",
      "./swing/202302121857/source ./swing/202302121857/target\n",
      "./swing/202302142128\n",
      "./swing/202302142128/source ./swing/202302142128/target\n",
      "./swing/202302121920\n",
      "./swing/202302121920/source ./swing/202302121920/target\n",
      "./swing/202302121909\n",
      "./swing/202302121909/source ./swing/202302121909/target\n",
      "./swing/202302142331\n",
      "./swing/202302142331/source ./swing/202302142331/target\n"
     ]
    }
   ],
   "source": [
    "front_pocket_pair_data = load_pair_data('./front_pocket', class_num=1)\n",
    "pocket_pair_data = load_pair_data('./pocket', class_num=2)\n",
    "swing_pair_data = load_pair_data('./swing', class_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73a5c707-8a25-4a06-874d-df286b5599c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433 413 406\n"
     ]
    }
   ],
   "source": [
    "print(len(front_pocket_pair_data), len(pocket_pair_data), len(swing_pair_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d2f40e0-0448-490c-9449-232c6a55bd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.89924149e+00,  1.56013069e+01,  9.66989136e-02, -3.46604615e-01,\n",
       "        -1.22895777e+00,  5.17699718e-01,  1.15293147e+00,  1.78475031e+00,\n",
       "        -1.25223210e+00,  1.02946167e+01, -1.43725891e+01,  4.03854370e+01,\n",
       "         1.67575812e+09]),\n",
       " array([-9.11454620e-01,  9.59669357e+00,  5.45929733e+00,  1.94446683e-01,\n",
       "        -3.76825482e-02, -4.20625433e-02, -3.26001422e-01,  1.31789166e+00,\n",
       "         7.59824039e-01, -3.69051743e+01,  1.47752380e+00, -2.53529663e+01,\n",
       "         1.67575811e+09]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front_pocket_pair_data[0][0][0, -1], front_pocket_pair_data[0][2][0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46515db-8a56-47e9-8bbe-8283de894d0c",
   "metadata": {},
   "source": [
    "# 建立dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "073af591-76e1-47ec-a306-252417490eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c060fb2-215e-4785-a9c5-5b05f4ee8bba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ClassDataset(Dataset):\n",
    "#     def __init__(self, data, label):\n",
    "#         self.data = data\n",
    "#         self.label = label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6048176-d0ec-4f39-aa20-afd7b53758da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, source_data, source_label, target_data, target_label, source_split=None, target_split=None):\n",
    "        self.source_data = source_data\n",
    "        self.source_label = source_label\n",
    "        self.target_data = target_data\n",
    "        self.target_label = target_label\n",
    "        self.source_split = source_split\n",
    "        self.target_split = target_split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_data[idx], self.source_label[idx], self.target_data[idx], self.target_label[idx], self.source_split[idx], self.target_split[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4733b36-7de2-49e6-ac7f-a58f9e9917ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tgt_mask(size) -> torch.tensor:\n",
    "    # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "    mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "    mask = mask.float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "    mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "\n",
    "    # EX for size=5:\n",
    "    # [[0., -inf, -inf, -inf, -inf],\n",
    "    #  [0.,   0., -inf, -inf, -inf],\n",
    "    #  [0.,   0.,   0., -inf, -inf],\n",
    "    #  [0.,   0.,   0.,   0., -inf],\n",
    "    #  [0.,   0.,   0.,   0.,   0.]]\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ada036cd-d67b-4d25-b935-0cdcde61f750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, chunk_size=5, seq_len=21, num_of_classes=2):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_of_classes = num_of_classes\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(9, 16, kernel_size=5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(16, 16, kernel_size=5),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        # 16 * (seq_len-8)\n",
    "        self.rnn = nn.RNN(input_size=16 * (seq_len - 8), hidden_size=64, num_layers=2, batch_first=True)\n",
    "        \n",
    "        self.last = nn.Sequential(\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, num_of_classes),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):  # input: (bs, chunk_size, seq_len, 9)\n",
    "        h = torch.reshape(x, (len(x) * self.chunk_size, self.seq_len, -1))  # (bs, chunk_size, seq_len, 9) -> (bs * chunk_size, seq_len, 9)\n",
    "        h = torch.permute(h, (0, 2, 1))  # (bs * chunk_size, seq_len, 9) -> (bs * chunk_size, 9, seq_len) [CNN要對最後一個維度做卷積]\n",
    "        \n",
    "        h = self.cnn(h)\n",
    "\n",
    "        h = torch.permute(h, (0, 2, 1))\n",
    "        h = torch.reshape(h, (len(x), self.chunk_size, -1))\n",
    "        \n",
    "        hz, _ = self.rnn(h)\n",
    "        out = self.last(hz)\n",
    "\n",
    "        out = torch.reshape(out, (len(x), self.chunk_size, self.num_of_classes))\n",
    "        \n",
    "        return out, hz\n",
    "    \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :].to(x.device)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class NotSimpleTransformer(nn.Module):\n",
    "    def __init__(self, chunk_size=5, seq_len=21, num_of_classes=2):\n",
    "        super(NotSimpleTransformer, self).__init__()\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_of_classes = num_of_classes\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(64 * self.seq_len, 0.1)\n",
    "        \n",
    "        ################\n",
    "        ### Imu Part ###\n",
    "        ################\n",
    "        #self.classifier = Classifier(chunk_size, seq_len, num_of_classes)\n",
    "        \n",
    "        ################\n",
    "        ### Mag Part ###\n",
    "        ################\n",
    "        self.imu_embedding = nn.Sequential(\n",
    "            nn.Linear(9, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.mag_layer = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.mag_transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=64 * self.seq_len, nhead=8, dropout=0.1, batch_first=True), num_layers=4)\n",
    "        self.mag_transformer_decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model=64 * self.seq_len, nhead=8, dropout=0.1, batch_first=True), num_layers=4)\n",
    "        self.mag_last = nn.Sequential(\n",
    "            nn.Linear(64 * self.seq_len, 16 * self.seq_len),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16 * self.seq_len, 3 * self.seq_len),\n",
    "        )\n",
    "        \n",
    "    def forward(self, source_imu, source_mag, target_mag=None):\n",
    "\n",
    "        if target_mag != None:\n",
    "            ################\n",
    "            ### imu part ###\n",
    "            ################\n",
    "#             source_h = self.imu_layer(source_imu)\n",
    "#             source_hz, _ = self.imu_rnn(source_h)\n",
    "#             source_predict_probability = self.imu_last(source_hz[:, -1])\n",
    "\n",
    "            ################\n",
    "            ### mag part ###\n",
    "            ################\n",
    "            source_mag_h = self.mag_layer(source_mag)  # (batch, chunk_size, seq_len, 3) -> (batch, chunk_size, seq_len, 64)\n",
    "            source_mag_h = torch.reshape(source_mag_h, (len(source_mag), self.chunk_size, -1))  # (batch, chunk_size, seq_len, 64) -> (batch, chunk_size, seq_len * 64)\n",
    "            source_mag_h = self.pos_encoder(source_mag_h)  # (batch, chunk_size, seq_len * 64)\n",
    "            source_imu_h = self.imu_embedding(source_imu)\n",
    "            source_imu_h = torch.reshape(source_imu_h, (len(source_imu), self.chunk_size, -1))\n",
    "            source_mag_h += source_imu_h\n",
    "            \n",
    "            target_mag = torch.concat([torch.zeros(len(target_mag), 1, self.seq_len, 3).to(source_mag.device), target_mag], dim=1)\n",
    "            tgt = self.mag_layer(target_mag)  # (batch, chunk_size, seq_len, 3) -> (batch, chunk_size, seq_len, 64)\n",
    "            tgt = torch.reshape(tgt, (len(source_mag), self.chunk_size, -1))  # (batch, chunk_size, seq_len, 64) -> (batch, chunk_size, seq_len * 64)\n",
    "            tgt = self.pos_encoder(tgt)  # (batch, chunk_size, seq_len * 64)\n",
    "            \n",
    "            source_mag_hz= self.mag_transformer_encoder(source_mag_h)  # (batch, chunk_size, seq_len * 64) -> (batch, chunk_size, seq_len * 64)\n",
    "\n",
    "#             source_latent = torch.add(source_hz, source_mag_hz)  # (batch, chunk_size, 48)\n",
    "            tgt_mask = get_tgt_mask(self.chunk_size).to(source_mag.device)\n",
    "            mem_mask = get_tgt_mask(self.chunk_size).to(source_mag.device)\n",
    "            predict_mag_latent = self.mag_transformer_decoder(tgt, source_mag_hz, tgt_mask, mem_mask)  # (batch, chunk_size, seq_len * 64) -> (batch, chunk_size, seq_len * 64)\n",
    "            predict_mag = self.mag_last(predict_mag_latent)  # (batch, chunk_size, seq_len * 64) -> (batch, chunk_size, seq_len * 3)\n",
    "            predict_mag = torch.reshape(predict_mag, (len(source_mag), self.chunk_size, self.seq_len, -1))  # (batch * chunk_size, seq_len, 3) -> (batch, chunk_size, seq_len, 3)\n",
    "\n",
    "            return predict_mag\n",
    "        \n",
    "        else:\n",
    "            ################\n",
    "            ### imu part ###\n",
    "            ################\n",
    "#             source_h = self.imu_layer(source_imu)\n",
    "#             source_hz, _ = self.imu_rnn(source_h)\n",
    "#             source_predict_probability = self.imu_last(source_hz[:, -1])\n",
    "\n",
    "            ################\n",
    "            ### mag part ###\n",
    "            ################\n",
    "            source_mag_h = self.mag_layer(source_mag)  # (batch, chunk_size, seq_len, 3) -> (batch, chunk_size, seq_len, 64)\n",
    "            source_mag_h = torch.reshape(source_mag_h, (len(source_mag), self.chunk_size, -1))  # (batch, chunk_size, seq_len, 64) -> (batch, chunk_size, seq_len * 64)\n",
    "            source_mag_h = self.pos_encoder(source_mag_h)  # (batch, chunk_size, seq_len * 64)\n",
    "            source_imu_h = self.imu_embedding(source_imu)\n",
    "            source_imu_h = torch.reshape(source_imu_h, (len(source_imu), self.chunk_size, -1))\n",
    "            source_mag_h += source_imu_h\n",
    "            source_mag_hz= self.mag_transformer_encoder(source_mag_h)  # (batch, chunk_size, seq_len * 64) -> (batch, chunk_size, seq_len * 64)\n",
    "\n",
    "#             source_latent = torch.add(source_hz, source_mag_hz)  # (batch, chunk_size, 48)\n",
    "            tgt = torch.zeros(len(source_mag), 1, self.seq_len, 3).to(source_mag.device)\n",
    "            tgt = self.mag_layer(tgt)  # (batch, chunk_size, seq_len, 3) -> (batch, chunk_size, seq_len, 64)\n",
    "            tgt = torch.reshape(tgt, (len(source_mag), 1, -1))\n",
    "        \n",
    "            for i in range(source_mag_hz.size(1)):\n",
    "#                 tgt_pos = self.pos_encoder(torch.zeros(source_mag_hz.size(0), 1, source_mag_hz.size(-1)).to(source_mag_hz.device) + i)\n",
    "                tgt_pos = self.pos_encoder(tgt.clone())\n",
    "                tgt_mask = get_tgt_mask(tgt.size(1)).to(source_mag.device)\n",
    "                decode_position = self.mag_transformer_decoder(tgt_pos[:, max(0, i-5):, :], source_mag_hz[:, max(0, i-5):i+1, :], tgt_mask[:, max(0, i-5):, :], tgt_mask[:, max(0, i-5):, :], memory_key_padding_mask=None)\n",
    "                tgt = torch.concat([tgt, decode_position[:, -1:]], dim=1)\n",
    "            predict_mag = self.mag_last(tgt)  # (batch, chunk_size, seq_len * 64) -> (batch, chunk_size, seq_len * 3)\n",
    "            predict_mag = torch.reshape(predict_mag, (len(source_mag), self.chunk_size + 1, self.seq_len, -1))  # (batch * chunk_size, seq_len, 3) -> (batch, chunk_size, seq_len, 3)\n",
    "            \n",
    "            return predict_mag[:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a9b697b-6b12-4dd2-a2bf-5d9242762705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, chunk_size=5, seq_len=21):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=32 * self.seq_len, hidden_size=32 * self.seq_len, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.lstm = nn.LSTM(input_size=32 * self.seq_len, hidden_size=32 * self.seq_len, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.discriminant_layer = nn.Sequential(\n",
    "            nn.Linear(64 * self.seq_len, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.feature_extractor(x)  # (bs, chunk_size, seq_len, 3) -> (bs, chunk_size, seq_len, 16)\n",
    "        \n",
    "        h = h.reshape((len(x), self.chunk_size, -1))  # (bs, chunk_size, seq_len, 16) -> (bs, chunk_size, seq_len * 16)\n",
    "        hz, _ = self.lstm(h)  # (bs, chunk_size, seq_len * 16) -> (bs, chunk_size, seq_len * 16 * 2)\n",
    "        \n",
    "        out = self.discriminant_layer(hz)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24cb3f83-f508-4def-90c2-f0cd46743538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstDerivativeLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean', chunk_size=5):\n",
    "        super(FirstDerivativeLoss, self).__init__()\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        # calculate the first derivative\n",
    "        source_o = torch.reshape(source, (len(source), self.chunk_size, -1))\n",
    "        target_o = torch.reshape(target, (len(target), self.chunk_size, -1))\n",
    "        d_source = source_o[1:] - source_o[:-1]\n",
    "        d_target = target_o[1:] - target_o[:-1]\n",
    "        deriv = d_source - d_target\n",
    "\n",
    "        # calculate the loss as the mean squared error of the derivative\n",
    "        loss = torch.mean(torch.pow(deriv, 2))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30960412-1af6-47ed-90d8-0c8a7dc4a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(gt_mag, pred_mag, d_res):\n",
    "    pred_loss = mse_loss(pred_mag, gt_mag)\n",
    "    d_loss = bce_loss(d_res, torch.ones_like(d_res))\n",
    "    d1_loss = div_loss(pred_mag, gt_mag)\n",
    "\n",
    "    return pred_loss + d_loss*50 + d1_loss*3, pred_loss, d1_loss\n",
    "\n",
    "def discriminator_loss(d_real, d_fake):\n",
    "    real_loss = bce_loss(d_real, torch.ones_like(d_real))\n",
    "    fake_loss = bce_loss(d_fake, torch.zeros_like(d_fake))\n",
    "\n",
    "    return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9366367b-10db-4dd5-972b-eb89ad061ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "front_pocket_half = int(len(front_pocket_pair_data) * 0.8)\n",
    "pocket_half = int(len(pocket_pair_data) * 0.8)\n",
    "swing_half = int(len(swing_pair_data) * 0.8)\n",
    "\n",
    "train_data = front_pocket_pair_data[:front_pocket_half] + pocket_pair_data[:pocket_half] + swing_pair_data[:swing_half]\n",
    "valid_data = front_pocket_pair_data[front_pocket_half:] + pocket_pair_data[pocket_half:] + swing_pair_data[swing_half:]\n",
    "\n",
    "# train\n",
    "train_source_data = np.array([d[0] for d in train_data])\n",
    "train_source_label = np.array([d[1] for d in train_data])\n",
    "train_target_data = np.array([d[2] for d in train_data])\n",
    "train_target_label = np.array([d[3] for d in train_data])\n",
    "train_dataset = PairDataset(\n",
    "                    source_data = torch.tensor(train_source_data, dtype=torch.float),\n",
    "                    source_label = train_source_label,\n",
    "                    target_data = torch.tensor(train_target_data, dtype=torch.float),\n",
    "                    target_label = train_target_label,\n",
    "                    source_split = torch.tensor(np.array([d[4] for d in train_data]), dtype=torch.float),\n",
    "                    target_split = torch.tensor(np.array([d[5] for d in train_data]), dtype=torch.float),\n",
    "                )\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# valid\n",
    "valid_source_data = np.array([d[0] for d in valid_data])\n",
    "valid_source_label = np.array([d[1] for d in valid_data])\n",
    "valid_target_data = np.array([d[2] for d in valid_data])\n",
    "valid_target_label = np.array([d[3] for d in valid_data])\n",
    "valid_dataset = PairDataset(\n",
    "                    source_data = torch.tensor(valid_source_data, dtype=torch.float),\n",
    "                    source_label = valid_source_label,\n",
    "                    target_data = torch.tensor(valid_target_data, dtype=torch.float),\n",
    "                    target_label = valid_target_label,\n",
    "                    source_split = torch.tensor(np.array([d[4] for d in valid_data]), dtype=torch.float),\n",
    "                    target_split = torch.tensor(np.array([d[5] for d in valid_data]), dtype=torch.float),\n",
    "                )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b88b083b-cc71-47d3-8491-780712b263eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72fc51ec-0355-4e62-8804-94684bfb87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1674672d-763d-47dc-a44c-7e7820893068",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 2000\n",
    "num_of_classes = 4\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "bce_loss = torch.nn.BCELoss()\n",
    "div_loss = FirstDerivativeLoss(chunk_size=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31e23ee4-8da1-4fc7-bbd9-0b2397de5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NotSimpleTransformer(chunk_size=chunk_size, seq_len=seq_len, num_of_classes=num_of_classes).to(device)\n",
    "discriminator = Discriminator(chunk_size=chunk_size, seq_len=seq_len).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.000025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fec0d49-bec9-4777-be88-ba2261557cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(result):\n",
    "    pred_mag = result[0].reshape(-1, 3)\n",
    "    targ_mag = result[1].reshape(-1, 3)\n",
    "    x = np.arange(len(pred_mag))\n",
    "    \n",
    "    loss = mse_loss(torch.tensor(pred_mag), torch.tensor(targ_mag)).item()\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for pm, tm, caxis, color in zip(pred_mag.T, targ_mag.T, ['x', 'y', 'z'], ['tab:blue', 'tab:orange', 'tab:green']):\n",
    "        plt.plot(x, pm, label=f'pred_{caxis}', color=color, linewidth=2.5, zorder=2)\n",
    "        plt.plot(x, tm, '--', label=f'targ_{caxis}', color=color, linewidth=1, alpha=0.8, zorder=1)\n",
    "    \n",
    "    for i in range(0, chunk_size+1):\n",
    "        plt.axvline(x=int(i * seq_len - 0.5), linewidth=0.5, linestyle='-', color='k', alpha=0.5)\n",
    "    \n",
    "    plt.suptitle(f'Loss: {loss:.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ae9ae48-8ffb-470a-8502-2a9ba436e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion(pred_mags):\n",
    "    cks = chunk_size + 1\n",
    "    half_len = int(seq_len / 2)\n",
    "    fusion_mags = torch.zeros(len(pred_mags), cks, half_len, 3).to(pred_mags.device)\n",
    "    \n",
    "    for i in range(cks - 1):\n",
    "        fusion_mags[:, i:i+2] += pred_mags[:, i].reshape(len(pred_mags), 2, half_len, 3)\n",
    "        \n",
    "    fusion_mags[:, 1:cks-1] = fusion_mags[:, 1:cks-1] / 2\n",
    "    return fusion_mags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eba50865-9a85-43cb-8719-61f8868194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, draw=False):\n",
    "    model.train()\n",
    "\n",
    "    pred_losses = []\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    d1_losses = []\n",
    "\n",
    "    for source_data, source_label, target_data, target_label in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        source_data = source_data.to(device)\n",
    "        target_data = target_data.to(device)\n",
    "\n",
    "        one_hot = F.one_hot(source_label, num_classes=num_of_classes).to(device).float()\n",
    "\n",
    "        # generate mag\n",
    "        predict_mag = model(source_data[:, :, :, :9], source_data[:, :, :, 9:12], target_data[:, :-1, :, 9:12])\n",
    "        predict_mag = fusion(predict_mag)\n",
    "        \n",
    "        # generator loss\n",
    "        d_res = discriminator(predict_mag)\n",
    "        gen_loss, pred_loss, d1_loss = generator_loss(target_data[:, :, :, 9:12], predict_mag, d_res)\n",
    "        \n",
    "        # backward\n",
    "        gen_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # discriminator loss\n",
    "        d_real = discriminator(target_data[:, :, :, 9:12])\n",
    "        d_fake = discriminator(predict_mag.detach())\n",
    "        dis_loss = discriminator_loss(d_real, d_fake)\n",
    "\n",
    "        # backward\n",
    "        dis_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # record loss\n",
    "        pred_losses.append(pred_loss.item())\n",
    "        g_losses.append(gen_loss.item())\n",
    "        d1_losses.append(d1_loss.item())\n",
    "        d_losses.append(dis_loss.item())\n",
    "    \n",
    "    if draw:\n",
    "        rs = [predict_mag[0].detach().cpu().numpy(), target_data[0, :, :, 9:12].cpu().numpy()]\n",
    "        plot_result(rs)\n",
    "    \n",
    "    return np.mean(pred_losses), np.mean(g_losses), np.mean(d1_losses), np.mean(d_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cb16192-e16c-4d77-bc8f-7d1b6eff1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute(dataloader, draw=False):\n",
    "    model.eval()\n",
    "\n",
    "    pred_losses = []\n",
    "    d1_losses = []\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for source_data, source_label, target_data, target_label in dataloader:\n",
    "            source_data = source_data.to(device)\n",
    "            target_data = target_data.to(device)\n",
    "            \n",
    "            one_hot = F.one_hot(source_label, num_classes=num_of_classes).to(device).float()\n",
    "\n",
    "            # generate mag\n",
    "            predict_mag = model(source_data[:, :, :, :9], source_data[:, :, :, 9:12], target_data[:, :-1, :, 9:12])\n",
    "            predict_mag = fusion(predict_mag)\n",
    "            \n",
    "            # generator loss\n",
    "            d_res = discriminator(predict_mag)\n",
    "            gen_loss, pred_loss, d1_loss= generator_loss(target_data[:, :, :, 9:12], predict_mag, d_res)\n",
    "\n",
    "            # discriminator loss\n",
    "            d_real = discriminator(target_data[:, :, :, 9:12])\n",
    "            d_fake = discriminator(predict_mag.detach())\n",
    "            dis_loss = discriminator_loss(d_real, d_fake)\n",
    "\n",
    "            # record loss\n",
    "            pred_losses.append(pred_loss.item())\n",
    "            g_losses.append(gen_loss.item())\n",
    "            d1_losses.append(d1_loss.item())\n",
    "            d_losses.append(dis_loss.item())\n",
    "    \n",
    "    if draw:\n",
    "        rs = [predict_mag[0].detach().cpu().numpy(), target_data[0, :, :, 9:12].cpu().numpy()]\n",
    "        plot_result(rs)\n",
    "    \n",
    "    return np.mean(pred_losses), np.mean(g_losses), np.mean(d1_losses), np.mean(d_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5e74f03-1f6e-4245-976b-7ef42b7c366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 10, -1]' is invalid for input of size 135168",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m     draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m train_pred_loss, train_g_loss, train_diff_loss, train_d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m valid_pred_loss, valid_g_loss, valid_diff_loss, valid_d_loss \u001b[38;5;241m=\u001b[39m evalute(valid_loader, draw)\n\u001b[1;32m     16\u001b[0m draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, draw)\u001b[0m\n\u001b[1;32m     19\u001b[0m predict_mag \u001b[38;5;241m=\u001b[39m fusion(predict_mag)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# generator loss\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m d_res \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_mag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m gen_loss, pred_loss, d1_loss \u001b[38;5;241m=\u001b[39m generator_loss(target_data[:, :, :, \u001b[38;5;241m9\u001b[39m:\u001b[38;5;241m12\u001b[39m], predict_mag, d_res)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# backward\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mag39/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     26\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(x)  \u001b[38;5;66;03m# (bs, chunk_size, seq_len, 3) -> (bs, chunk_size, seq_len, 16)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, chunk_size, seq_len, 16) -> (bs, chunk_size, seq_len * 16)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     hz, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(h)  \u001b[38;5;66;03m# (bs, chunk_size, seq_len * 16) -> (bs, chunk_size, seq_len * 16 * 2)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminant_layer(hz)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 10, -1]' is invalid for input of size 135168"
     ]
    }
   ],
   "source": [
    "draw = True\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    #####\n",
    "    # 1. 用上半部訓練50epoch\n",
    "    # 2. 隨機用上或下半部訓練Model\n",
    "    # 3. 印出trajectory結果\n",
    "    #####\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        draw = True\n",
    "    \n",
    "    train_pred_loss, train_g_loss, train_diff_loss, train_d_loss = train(train_loader, draw)\n",
    "    valid_pred_loss, valid_g_loss, valid_diff_loss, valid_d_loss = evalute(valid_loader, draw)\n",
    "    \n",
    "    draw = False\n",
    "    \n",
    "    ep = str(epoch).zfill(5)\n",
    "    print(f'{ep:>5}: train pred loss: {train_pred_loss: 2.3f}, g loss: {train_g_loss: 2.3f}, diff loss: {train_diff_loss: 2.3f}, d loss: {train_d_loss: 2.3f}\\n' +\n",
    "          f'{\"\":>5}  valid pred loss: {valid_pred_loss: 2.3f}, g loss: {valid_g_loss: 2.3f}, diff loss: {valid_diff_loss: 2.3f}, d loss: {valid_d_loss: 2.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d28d2b-c05c-4053-81ab-da651c91028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_eval(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for source_data, source_label, target_data, target_label in dataloader:\n",
    "            source_data = source_data.to(device)\n",
    "            target_data = target_data.to(device)\n",
    "\n",
    "            #############\n",
    "            # generator #\n",
    "            #############\n",
    "            predict_mag = model(source_data[:, :, :, :9], source_data[:, :, :, 9:12])\n",
    "\n",
    "            predict_loss = mse_loss(predict_mag, target_data[:, :, :, 9:12])\n",
    "            \n",
    "            results.extend(zip(predict_mag.detach().cpu().numpy(), target_data[:, :, :, 9:12].cpu().numpy()))\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939dee9b-ce00-4ab3-a471-02eeecec79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(result):\n",
    "    pred_mag = result[0].reshape(-1, 3)\n",
    "    targ_mag = result[1].reshape(-1, 3)\n",
    "    x = np.arange(len(pred_mag))\n",
    "    \n",
    "    loss = mse_loss(torch.tensor(pred_mag), torch.tensor(targ_mag)).item()\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for pm, tm, caxis, color in zip(pred_mag.T, targ_mag.T, ['x', 'y', 'z'], ['tab:blue', 'tab:orange', 'tab:green']):\n",
    "        plt.plot(x, pm, label=f'pred_{caxis}', color=color, linewidth=2.5, zorder=2)\n",
    "        plt.plot(x, tm, '--', label=f'targ_{caxis}', color=color, linewidth=1, alpha=0.8, zorder=1)\n",
    "    \n",
    "    for i in range(0, chunk_size+1):\n",
    "        plt.axvline(x=int(i * seq_len - 0.5), linewidth=0.5, linestyle='-', color='k', alpha=0.5)\n",
    "    \n",
    "    plt.suptitle(f'Loss: {loss:.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c0beb-8280-4a77-a23a-e421405bb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = output_eval(model, train_loader)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "for rs in results[::5]:\n",
    "    plot_result(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dca13b-1f6d-40dc-895d-8ce7403c954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = output_eval(model, valid_loader)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "for rs in results[::5]:\n",
    "    plot_result(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf58007-cc88-4b25-8844-b3fe898ad6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mag39",
   "language": "python",
   "name": "mag39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
