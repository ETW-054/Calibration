{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a92978-52d9-4cc8-b309-2a8874549ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c01a44-c290-4f08-8df4-97acc58764bf",
   "metadata": {},
   "source": [
    "# 解壓縮資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34a0f5a-04f9-4bfb-9154-2001904235c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_data(path):\n",
    "    for folder, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('zip'):\n",
    "                file_path = os.path.join(folder, file)\n",
    "                print(file_path)\n",
    "\n",
    "                sotre_path = os.path.join(folder, file.rsplit('.')[0])\n",
    "                # 開啟 ZIP 壓縮檔 \n",
    "                with zipfile.ZipFile(file_path, 'r') as zf:\n",
    "                    # 解壓縮所有檔案至 /my/folder 目錄\n",
    "                    zf.extractall(path=sotre_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cecf48e8-21b8-46cd-a24d-6d79e9857780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip_data('./swing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986b396b-062a-4ebb-b6a1-1de47dbbb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv(path):\n",
    "    acc_df = pd.read_csv(os.path.join(path, 'Accelerometer.csv'), delimiter=',')\n",
    "    gyo_df = pd.read_csv(os.path.join(path, 'Gyroscope.csv'), delimiter=',')\n",
    "    linacc_df = pd.read_csv(os.path.join(path, 'Linear Accelerometer.csv'), delimiter=',')\n",
    "    mag_df = pd.read_csv(os.path.join(path, 'Magnetometer.csv'), delimiter=',')\n",
    "    device_df = pd.read_csv(os.path.join(path, 'meta', 'device.csv'), delimiter=',')\n",
    "    time_df = pd.read_csv(os.path.join(path, 'meta', 'time.csv'), delimiter=',')\n",
    "    \n",
    "    acc_df.to_csv(os.path.join(path, 'Accelerometer.csv'), index=False, sep=';')\n",
    "    gyo_df.to_csv(os.path.join(path, 'Gyroscope.csv'), index=False, sep=';')\n",
    "    linacc_df.to_csv(os.path.join(path, 'Linear Accelerometer.csv'), index=False, sep=';')\n",
    "    mag_df.to_csv(os.path.join(path, 'Magnetometer.csv'), index=False, sep=';')\n",
    "    device_df.to_csv(os.path.join(path, 'meta', 'device.csv'), index=False, sep=';')\n",
    "    time_df.to_csv(os.path.join(path, 'meta', 'time.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a400be-465b-46d6-8920-99424db47df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_csv('./pocket/202301101952/target')\n",
    "# convert_csv('./pocket/202301101952/source')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d0f88-db27-4381-bfa6-8a7f86930454",
   "metadata": {},
   "source": [
    "# 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b5f5c14-bd7c-4fb4-99c0-0a0cb769baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_data(df):\n",
    "    new_names = ['system_time', 'acc_times', 'acc_x', 'acc_y', 'acc_z', 'gyo_times', 'gyo_x', 'gyo_y', 'gyo_z', 'lin_acc_times', 'lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'mag_times', 'mag_x', 'mag_y', 'mag_z']\n",
    "    df.columns = new_names\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def device_start_system_time(path):\n",
    "    time_df = pd.read_csv(path, delimiter=';', index_col=0)\n",
    "    time = time_df.T.loc['system time', 'START']\n",
    "    \n",
    "    return time\n",
    "\n",
    "\n",
    "def load_original_data(path):\n",
    "    acc_df = pd.read_csv(os.path.join(path, 'Accelerometer.csv'), delimiter=';')\n",
    "    gyo_df = pd.read_csv(os.path.join(path, 'Gyroscope.csv'), delimiter=';')\n",
    "    linacc_df = pd.read_csv(os.path.join(path, 'Linear Accelerometer.csv'), delimiter=';')\n",
    "    mag_df = pd.read_csv(os.path.join(path, 'Magnetometer.csv'), delimiter=';')\n",
    "    start_time = device_start_system_time(os.path.join(path, 'meta/time.csv'))\n",
    "    time_df = acc_df.iloc[:, 0] + start_time\n",
    "    \n",
    "    total_df = pd.concat([time_df, acc_df, gyo_df, linacc_df, mag_df], axis=1)\n",
    "    total_df = rename_data(total_df)\n",
    "    \n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50962f15-f38a-429a-b00a-2adf9a6eb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_data(source_df, target_df):\n",
    "    source_start_time = source_df.loc[0, 'system_time']\n",
    "    target_start_time = target_df.loc[0, 'system_time']\n",
    "    \n",
    "    # align start time\n",
    "    if source_start_time > target_start_time:  # source start time > target start time\n",
    "        target_start_idx = np.argmin(np.abs(target_df.system_time - source_start_time))\n",
    "        target_df = target_df.iloc[target_start_idx:].reset_index(drop=True)\n",
    "    else:  # source start time < target start time\n",
    "        source_start_idx = np.argmin(np.abs(source_df.system_time - target_start_time))\n",
    "        source_df = source_df.iloc[source_start_idx:].reset_index(drop=True)\n",
    "        \n",
    "    # align end idx\n",
    "    end_idx = min(len(source_df), len(target_df))\n",
    "    source_df = source_df.iloc[:end_idx]\n",
    "    target_df = target_df.iloc[:end_idx]\n",
    "    \n",
    "    return source_df, target_df\n",
    "\n",
    "\n",
    "def bound_range(df):\n",
    "    start = datapoint_per_second * 35\n",
    "    end = len(df) - datapoint_per_second * 20\n",
    "    \n",
    "    return df.iloc[start:end].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def split_segments(df, seq_len=5, chunk_size=26):\n",
    "#     length = datapoint_per_second * duration\n",
    "    length = seq_len * chunk_size\n",
    "    num_of_segs = int(np.floor(len(df) / length))\n",
    "    \n",
    "    segments = []\n",
    "    for i in range(num_of_segs):\n",
    "        seg = df.iloc[int(i * length):int((i + 1) * length)].to_numpy()\n",
    "        segments.append(np.array(np.split(seg, seq_len)))\n",
    "        \n",
    "    return segments\n",
    "\n",
    "\n",
    "def select_data(df):\n",
    "    return df[['acc_x', 'acc_y', 'acc_z', 'gyo_x', 'gyo_y', 'gyo_z', 'lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'mag_x', 'mag_y', 'mag_z', 'system_time']]\n",
    "\n",
    "\n",
    "def preprocess_data(df, duration):\n",
    "    pre_df = select_data(df)\n",
    "    segs = split_segments(pre_df, 5)\n",
    "    \n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e99a24c-4c63-4e78-ae26-446adf71d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = load_original_data('./front_pocket/202302071724/source')\n",
    "# segs = preprocess_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57b7b75-a898-49d3-8b69-a7ac8fe74114",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint_per_second = 20\n",
    "duration = 2\n",
    "classes = {'target': 0, 'front_pocket': 1, 'pocket': 2, 'swing': 3}\n",
    "\n",
    "def device_version(path):\n",
    "    device_df = pd.read_csv(path, delimiter=';', index_col=0)\n",
    "    version = device_df.loc['deviceRelease'].value\n",
    "    \n",
    "    return version\n",
    "\n",
    "\n",
    "def check_data_device(source_path, target_path):\n",
    "    while True:\n",
    "        source_version = device_version(os.path.join(source_path, 'meta/device.csv'))\n",
    "        target_version = device_version(os.path.join(target_path, 'meta/device.csv'))\n",
    "\n",
    "        print(source_path, target_path)\n",
    "\n",
    "        if source_version[:2] == '15' and target_version[:2] == '16':\n",
    "            return source_path, target_path\n",
    "        elif source_version[:2] == '16' and target_version[:2] == '15':\n",
    "            source_path = os.path.join(folder_path, 'target')\n",
    "            target_path = os.path.join(folder_path, 'source')\n",
    "            print('--- GG ---')\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def load_pair_data(root_folder, class_num):\n",
    "    pair_data = []\n",
    "\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        source_path = os.path.join(folder_path, 'source')\n",
    "        target_path = os.path.join(folder_path, 'target')\n",
    "        \n",
    "        print(folder_path)\n",
    "        \n",
    "        #########################\n",
    "        ##### check devices #####\n",
    "        #########################\n",
    "        source_path, target_path = check_data_device(source_path, target_path)\n",
    "        \n",
    "        ####################################\n",
    "        ##### load and preprocess data #####\n",
    "        ####################################\n",
    "        source_df = load_original_data(source_path)\n",
    "        target_df = load_original_data(target_path)\n",
    "        \n",
    "        source_df, target_df = align_data(source_df, target_df)\n",
    "        source_df, target_df = bound_range(source_df), bound_range(target_df)\n",
    "        \n",
    "        source_segs = preprocess_data(source_df, duration)\n",
    "        target_segs = preprocess_data(target_df, duration)\n",
    "        \n",
    "        idx = min(len(source_segs), len(target_segs))\n",
    "        source_tags = [class_num] * idx\n",
    "        target_tags = [0] * idx\n",
    "        \n",
    "        pair_data.extend(zip(source_segs[:idx], source_tags, target_segs[:idx], target_tags))\n",
    "        \n",
    "    return pair_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34aa6ce4-9bb1-428a-b421-0d086fc4cba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15.4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_version('./front_pocket/202302071523/source/meta/device.csv')  # source version: 15.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "096e5ad0-4606-4ef4-aa30-ff27a1d21cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_version('./front_pocket/202302071523/target/meta/device.csv')  # target version: 16.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee569c5-fda8-44e6-be6b-3aff9efb2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./front_pocket/202302071628\n",
      "./front_pocket/202302071628/source ./front_pocket/202302071628/target\n",
      "./front_pocket/202302071652\n",
      "./front_pocket/202302071652/source ./front_pocket/202302071652/target\n",
      "./front_pocket/202302071523\n",
      "./front_pocket/202302071523/source ./front_pocket/202302071523/target\n",
      "./front_pocket/202302071531\n",
      "./front_pocket/202302071531/source ./front_pocket/202302071531/target\n",
      "./front_pocket/202302071715\n",
      "./front_pocket/202302071715/source ./front_pocket/202302071715/target\n",
      "./front_pocket/202302071641\n",
      "./front_pocket/202302071641/source ./front_pocket/202302071641/target\n",
      "./front_pocket/202302071541\n",
      "./front_pocket/202302071541/source ./front_pocket/202302071541/target\n",
      "./front_pocket/202302071619\n",
      "./front_pocket/202302071619/source ./front_pocket/202302071619/target\n",
      "./front_pocket/202302071704\n",
      "./front_pocket/202302071704/source ./front_pocket/202302071704/target\n",
      "./front_pocket/202302071724\n",
      "./front_pocket/202302071724/source ./front_pocket/202302071724/target\n",
      "./pocket/202302132108\n",
      "./pocket/202302132108/source ./pocket/202302132108/target\n",
      "./pocket/202302131750\n",
      "./pocket/202302131750/source ./pocket/202302131750/target\n",
      "./pocket/202302131601\n",
      "./pocket/202302131601/source ./pocket/202302131601/target\n",
      "./pocket/202302071606\n",
      "./pocket/202302071606/source ./pocket/202302071606/target\n",
      "./pocket/202302132053\n",
      "./pocket/202302132053/source ./pocket/202302132053/target\n",
      "./pocket/202302122132\n",
      "./pocket/202302122132/source ./pocket/202302122132/target\n",
      "./pocket/202302132116\n",
      "./pocket/202302132116/source ./pocket/202302132116/target\n",
      "./pocket/202302131643\n",
      "./pocket/202302131643/source ./pocket/202302131643/target\n",
      "./pocket/202301101952\n",
      "./pocket/202301101952/source ./pocket/202301101952/target\n",
      "./pocket/202302132101\n",
      "./pocket/202302132101/source ./pocket/202302132101/target\n",
      "./swing/202302142339\n",
      "./swing/202302142339/source ./swing/202302142339/target\n",
      "./swing/202302132131\n",
      "./swing/202302132131/source ./swing/202302132131/target\n",
      "./swing/202302142117\n",
      "./swing/202302142117/source ./swing/202302142117/target\n",
      "./swing/202302132124\n",
      "./swing/202302132124/source ./swing/202302132124/target\n",
      "./swing/202302121947\n",
      "./swing/202302121947/source ./swing/202302121947/target\n",
      "./swing/202302121857\n",
      "./swing/202302121857/source ./swing/202302121857/target\n",
      "./swing/202302142128\n",
      "./swing/202302142128/source ./swing/202302142128/target\n",
      "./swing/202302121920\n",
      "./swing/202302121920/source ./swing/202302121920/target\n",
      "./swing/202302121909\n",
      "./swing/202302121909/source ./swing/202302121909/target\n",
      "./swing/202302142331\n",
      "./swing/202302142331/source ./swing/202302142331/target\n"
     ]
    }
   ],
   "source": [
    "front_pocket_pair_data = load_pair_data('./front_pocket', class_num=1)\n",
    "pocket_pair_data = load_pair_data('./pocket', class_num=2)\n",
    "swing_pair_data = load_pair_data('./swing', class_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a5c707-8a25-4a06-874d-df286b5599c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441 420 412\n"
     ]
    }
   ],
   "source": [
    "print(len(front_pocket_pair_data), len(pocket_pair_data), len(swing_pair_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46515db-8a56-47e9-8bbe-8283de894d0c",
   "metadata": {},
   "source": [
    "# 建立dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "073af591-76e1-47ec-a306-252417490eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d14471-de4a-4918-90be-e07462c99c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ada036cd-d67b-4d25-b935-0cdcde61f750",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, seq_len=5, chunk_size=21, num_of_classes=2):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "#         self.feature_extractor = nn.Sequential(\n",
    "#             nn.Linear(9, 16),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(16, 16),\n",
    "#             nn.LeakyReLU(),\n",
    "#         )\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, kernel_size=(1, 5)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(5, 5, kernel_size=(1, 5)),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=13, hidden_size=13, num_layers=2, batch_first=True)\n",
    "#         self.lstm = nn.LSTM(input_size=16, hidden_size=16, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.last = nn.Sequential(\n",
    "            nn.Linear(13, 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, num_of_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         h = self.feature_extractor(x)\n",
    "        print(x.shape)\n",
    "        h = torch.reshape(x, (len(x) * self.seq_len, self.chunk_size, -1))\n",
    "        print(h.shape)\n",
    "        h = torch.permute(h, (0, 2, 1))[:, None, :]\n",
    "        print(h.shape)\n",
    "        \n",
    "        h = self.cnn(h)\n",
    "        print(h.shape)\n",
    "        h = torch.permute(h, (0, 2, 1))\n",
    "        h = torch.reshape(h, (len(x), self.seq_len, -1))\n",
    "        hz, _ = self.rnn(h)\n",
    "        \n",
    "        out = self.last(hz[:, -1])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce7068eb-49fa-403b-bee7-ebac138af819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, seq_len=5, chunk_size=21, num_of_classes=2):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_of_classes = num_of_classes\n",
    "        \n",
    "#         self.feature_extractor = nn.Sequential(\n",
    "#             nn.Linear(9, 16),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(16, 16),\n",
    "#             nn.LeakyReLU(),\n",
    "#         )\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(9, 16, kernel_size=5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(16, 16, kernel_size=5),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        # 16 * (chunk_size-8)\n",
    "        self.rnn = nn.RNN(input_size=16 * (chunk_size - 8), hidden_size=64, num_layers=2, batch_first=True)\n",
    "#         self.lstm = nn.LSTM(input_size=16, hidden_size=16, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.last = nn.Sequential(\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, num_of_classes),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         h = self.feature_extractor(x)\n",
    "        h = torch.reshape(x, (len(x) * self.seq_len, self.chunk_size, -1))\n",
    "        h = torch.permute(h, (0, 2, 1))\n",
    "        \n",
    "        h = self.cnn(h)\n",
    "\n",
    "        h = torch.permute(h, (0, 2, 1))\n",
    "        h = torch.reshape(h, (len(x), self.seq_len, -1))\n",
    "        \n",
    "        hz, _ = self.rnn(h)\n",
    "        out = self.last(hz)\n",
    "\n",
    "        out = torch.reshape(out, (len(x), self.seq_len, self.num_of_classes))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9366367b-10db-4dd5-972b-eb89ad061ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0, 1, 2, 3]), array([1, 1, 1, 1])),\n",
       " (array([0, 1, 2, 3]), array([1270,  440,  419,  411])))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front_pocket_half = min(int(len(front_pocket_pair_data) * 0.5), 1)\n",
    "pocket_half = min(int(len(pocket_pair_data) * 0.5), 1)\n",
    "swing_half = min(int(len(swing_pair_data) * 0.5), 1)\n",
    "\n",
    "train_data = front_pocket_pair_data[:front_pocket_half] + pocket_pair_data[:pocket_half] + swing_pair_data[:swing_half]\n",
    "valid_data = front_pocket_pair_data[front_pocket_half:] + pocket_pair_data[pocket_half:] + swing_pair_data[swing_half:]\n",
    "\n",
    "# train\n",
    "t_data = np.array([d[0] for d in train_data] + [d[2] for d in train_data[::3]])\n",
    "t_label = [d[1] for d in train_data] + [d[3] for d in train_data[::3]]\n",
    "train_dataset = ClassDataset(\n",
    "                    data = torch.tensor(t_data, dtype=torch.float),\n",
    "                    label = t_label,\n",
    "                )\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# valid\n",
    "v_data = np.array([d[0] for d in valid_data] + [d[2] for d in valid_data])\n",
    "v_label = [d[1] for d in valid_data] + [d[3] for d in valid_data]\n",
    "valid_dataset = ClassDataset(\n",
    "                    data = torch.tensor(v_data, dtype=torch.float),\n",
    "                    label = v_label,\n",
    "                )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "\n",
    "np.unique(t_label, return_counts=True), np.unique(v_label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b88b083b-cc71-47d3-8491-780712b263eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c26b1c8-4264-4712-be81-8514544ade18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 26, 13])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1674672d-763d-47dc-a44c-7e7820893068",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 500\n",
    "num_of_classes = 4\n",
    "seq_len = 5\n",
    "chunk_size = 26\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "31e23ee4-8da1-4fc7-bbd9-0b2397de5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(seq_len=seq_len, chunk_size=chunk_size, num_of_classes=num_of_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0000001)\n",
    "ce_loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eba50865-9a85-43cb-8719-61f8868194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for sequences, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sequences = sequences.to(device)\n",
    "        one_hot = F.one_hot(labels, num_classes=num_of_classes).to(device).float()\n",
    "        one_hot = one_hot[:, None, :].repeat((1, seq_len, 1)).float()\n",
    "\n",
    "        #############\n",
    "        # generator #\n",
    "        #############\n",
    "        predict_probability = model(sequences[:, :, :, :9])\n",
    "        \n",
    "        predict_probability = predict_probability.reshape((-1, num_of_classes))\n",
    "        one_hot = one_hot.reshape((-1, num_of_classes))\n",
    "        loss = ce_loss(predict_probability, one_hot)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predict_classes = torch.max(predict_probability, 1)\n",
    "        predict_classes = predict_classes.cpu().detach().numpy()\n",
    "        _, labels = torch.max(one_hot, 1)\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(accuracy_score(labels, predict_classes))\n",
    "    \n",
    "    return np.mean(losses), np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8cb16192-e16c-4d77-bc8f-7d1b6eff1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in dataloader:\n",
    "            \n",
    "            sequences = sequences.to(device)\n",
    "            one_hot = F.one_hot(labels, num_classes=num_of_classes).to(device).float()\n",
    "            one_hot = one_hot[:, None, :].repeat((1, seq_len, 1)).float()\n",
    "\n",
    "            #############\n",
    "            # generator #\n",
    "            #############\n",
    "            predict_probability = model(sequences[:, :, :, :9])\n",
    "            \n",
    "            predict_probability = predict_probability.reshape((-1, num_of_classes))\n",
    "            one_hot = one_hot.reshape((-1, num_of_classes))\n",
    "            loss = ce_loss(predict_probability, one_hot)\n",
    "\n",
    "            _, predict_classes = torch.max(predict_probability, 1)\n",
    "            predict_classes = predict_classes.cpu().detach().numpy()\n",
    "            _, labels = torch.max(one_hot, 1)\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(accuracy_score(labels, predict_classes))\n",
    "    \n",
    "    return np.mean(losses), np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5e74f03-1f6e-4245-976b-7ef42b7c366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000: train loss: 1.379, acc: 0.350   valid loss: 1.376, acc: 0.240\n",
      "00001: train loss: 1.354, acc: 0.500   valid loss: 1.363, acc: 0.329\n",
      "00002: train loss: 1.327, acc: 0.700   valid loss: 1.351, acc: 0.404\n",
      "00003: train loss: 1.299, acc: 0.750   valid loss: 1.339, acc: 0.431\n",
      "00004: train loss: 1.271, acc: 0.800   valid loss: 1.327, acc: 0.488\n",
      "00005: train loss: 1.243, acc: 0.800   valid loss: 1.314, acc: 0.517\n",
      "00006: train loss: 1.217, acc: 0.800   valid loss: 1.302, acc: 0.546\n",
      "00007: train loss: 1.192, acc: 0.800   valid loss: 1.291, acc: 0.579\n",
      "00008: train loss: 1.169, acc: 0.850   valid loss: 1.279, acc: 0.608\n",
      "00009: train loss: 1.148, acc: 0.950   valid loss: 1.269, acc: 0.629\n",
      "00010: train loss: 1.129, acc: 0.950   valid loss: 1.258, acc: 0.646\n",
      "00011: train loss: 1.112, acc: 0.950   valid loss: 1.249, acc: 0.659\n",
      "00012: train loss: 1.096, acc: 0.950   valid loss: 1.240, acc: 0.672\n",
      "00013: train loss: 1.081, acc: 1.000   valid loss: 1.231, acc: 0.680\n",
      "00014: train loss: 1.067, acc: 1.000   valid loss: 1.222, acc: 0.686\n",
      "00015: train loss: 1.055, acc: 1.000   valid loss: 1.214, acc: 0.693\n",
      "00016: train loss: 1.043, acc: 1.000   valid loss: 1.207, acc: 0.697\n",
      "00017: train loss: 1.032, acc: 1.000   valid loss: 1.200, acc: 0.700\n",
      "00018: train loss: 1.023, acc: 1.000   valid loss: 1.194, acc: 0.702\n",
      "00019: train loss: 1.015, acc: 1.000   valid loss: 1.189, acc: 0.703\n",
      "00020: train loss: 1.007, acc: 1.000   valid loss: 1.184, acc: 0.705\n",
      "00021: train loss: 1.000, acc: 1.000   valid loss: 1.179, acc: 0.707\n",
      "00022: train loss: 0.994, acc: 1.000   valid loss: 1.174, acc: 0.711\n",
      "00023: train loss: 0.988, acc: 1.000   valid loss: 1.169, acc: 0.715\n",
      "00024: train loss: 0.983, acc: 1.000   valid loss: 1.163, acc: 0.722\n",
      "00025: train loss: 0.978, acc: 1.000   valid loss: 1.158, acc: 0.729\n",
      "00026: train loss: 0.974, acc: 1.000   valid loss: 1.154, acc: 0.733\n",
      "00027: train loss: 0.970, acc: 1.000   valid loss: 1.149, acc: 0.736\n",
      "00028: train loss: 0.966, acc: 1.000   valid loss: 1.145, acc: 0.741\n",
      "00029: train loss: 0.963, acc: 1.000   valid loss: 1.141, acc: 0.745\n",
      "00030: train loss: 0.960, acc: 1.000   valid loss: 1.137, acc: 0.748\n",
      "00031: train loss: 0.957, acc: 1.000   valid loss: 1.134, acc: 0.750\n",
      "00032: train loss: 0.955, acc: 1.000   valid loss: 1.131, acc: 0.752\n",
      "00033: train loss: 0.952, acc: 1.000   valid loss: 1.129, acc: 0.754\n",
      "00034: train loss: 0.951, acc: 1.000   valid loss: 1.126, acc: 0.756\n",
      "00035: train loss: 0.949, acc: 1.000   valid loss: 1.124, acc: 0.757\n",
      "00036: train loss: 0.947, acc: 1.000   valid loss: 1.123, acc: 0.758\n",
      "00037: train loss: 0.946, acc: 1.000   valid loss: 1.121, acc: 0.760\n",
      "00038: train loss: 0.945, acc: 1.000   valid loss: 1.120, acc: 0.761\n",
      "00039: train loss: 0.944, acc: 1.000   valid loss: 1.119, acc: 0.762\n",
      "00040: train loss: 0.943, acc: 1.000   valid loss: 1.118, acc: 0.762\n",
      "00041: train loss: 0.943, acc: 1.000   valid loss: 1.117, acc: 0.763\n",
      "00042: train loss: 0.942, acc: 1.000   valid loss: 1.117, acc: 0.763\n",
      "00043: train loss: 0.942, acc: 1.000   valid loss: 1.116, acc: 0.763\n",
      "00044: train loss: 0.942, acc: 1.000   valid loss: 1.116, acc: 0.764\n",
      "00045: train loss: 0.941, acc: 1.000   valid loss: 1.116, acc: 0.764\n",
      "00046: train loss: 0.941, acc: 1.000   valid loss: 1.116, acc: 0.764\n",
      "00047: train loss: 0.941, acc: 1.000   valid loss: 1.116, acc: 0.764\n",
      "00048: train loss: 0.941, acc: 1.000   valid loss: 1.116, acc: 0.764\n",
      "00049: train loss: 0.941, acc: 1.000   valid loss: 1.116, acc: 0.764\n",
      "00050: train loss: 0.941, acc: 1.000   valid loss: 1.116, acc: 0.764\n",
      "00051: train loss: 0.941, acc: 1.000   valid loss: 1.116, acc: 0.764\n",
      "00052: train loss: 0.941, acc: 1.000   valid loss: 1.116, acc: 0.764\n",
      "00053: train loss: 0.941, acc: 1.000   valid loss: 1.116, acc: 0.764\n",
      "00054: train loss: 0.941, acc: 1.000   valid loss: 1.115, acc: 0.764\n",
      "00055: train loss: 0.941, acc: 1.000   valid loss: 1.115, acc: 0.764\n",
      "00056: train loss: 0.941, acc: 1.000   valid loss: 1.115, acc: 0.764\n",
      "00057: train loss: 0.940, acc: 1.000   valid loss: 1.114, acc: 0.765\n",
      "00058: train loss: 0.940, acc: 1.000   valid loss: 1.114, acc: 0.765\n",
      "00059: train loss: 0.940, acc: 1.000   valid loss: 1.113, acc: 0.766\n",
      "00060: train loss: 0.939, acc: 1.000   valid loss: 1.112, acc: 0.767\n",
      "00061: train loss: 0.939, acc: 1.000   valid loss: 1.111, acc: 0.768\n",
      "00062: train loss: 0.938, acc: 1.000   valid loss: 1.110, acc: 0.769\n",
      "00063: train loss: 0.937, acc: 1.000   valid loss: 1.109, acc: 0.770\n",
      "00064: train loss: 0.936, acc: 1.000   valid loss: 1.107, acc: 0.771\n",
      "00065: train loss: 0.935, acc: 1.000   valid loss: 1.106, acc: 0.773\n",
      "00066: train loss: 0.934, acc: 1.000   valid loss: 1.104, acc: 0.775\n",
      "00067: train loss: 0.932, acc: 1.000   valid loss: 1.101, acc: 0.777\n",
      "00068: train loss: 0.931, acc: 1.000   valid loss: 1.099, acc: 0.780\n",
      "00069: train loss: 0.929, acc: 1.000   valid loss: 1.096, acc: 0.783\n",
      "00070: train loss: 0.927, acc: 1.000   valid loss: 1.093, acc: 0.785\n",
      "00071: train loss: 0.925, acc: 1.000   valid loss: 1.090, acc: 0.788\n",
      "00072: train loss: 0.923, acc: 1.000   valid loss: 1.086, acc: 0.793\n",
      "00073: train loss: 0.921, acc: 1.000   valid loss: 1.083, acc: 0.797\n",
      "00074: train loss: 0.919, acc: 1.000   valid loss: 1.079, acc: 0.802\n",
      "00075: train loss: 0.916, acc: 1.000   valid loss: 1.074, acc: 0.809\n",
      "00076: train loss: 0.914, acc: 1.000   valid loss: 1.070, acc: 0.818\n",
      "00077: train loss: 0.911, acc: 1.000   valid loss: 1.065, acc: 0.826\n",
      "00078: train loss: 0.908, acc: 1.000   valid loss: 1.061, acc: 0.835\n",
      "00079: train loss: 0.905, acc: 1.000   valid loss: 1.056, acc: 0.844\n",
      "00080: train loss: 0.903, acc: 1.000   valid loss: 1.051, acc: 0.852\n",
      "00081: train loss: 0.900, acc: 1.000   valid loss: 1.046, acc: 0.861\n",
      "00082: train loss: 0.897, acc: 1.000   valid loss: 1.040, acc: 0.869\n",
      "00083: train loss: 0.894, acc: 1.000   valid loss: 1.035, acc: 0.875\n",
      "00084: train loss: 0.891, acc: 1.000   valid loss: 1.030, acc: 0.882\n",
      "00085: train loss: 0.888, acc: 1.000   valid loss: 1.025, acc: 0.889\n",
      "00086: train loss: 0.885, acc: 1.000   valid loss: 1.020, acc: 0.893\n",
      "00087: train loss: 0.882, acc: 1.000   valid loss: 1.014, acc: 0.895\n",
      "00088: train loss: 0.879, acc: 1.000   valid loss: 1.009, acc: 0.898\n",
      "00089: train loss: 0.876, acc: 1.000   valid loss: 1.004, acc: 0.903\n",
      "00090: train loss: 0.873, acc: 1.000   valid loss: 0.999, acc: 0.909\n",
      "00091: train loss: 0.870, acc: 1.000   valid loss: 0.994, acc: 0.912\n",
      "00092: train loss: 0.867, acc: 1.000   valid loss: 0.989, acc: 0.913\n",
      "00093: train loss: 0.864, acc: 1.000   valid loss: 0.985, acc: 0.914\n",
      "00094: train loss: 0.862, acc: 1.000   valid loss: 0.980, acc: 0.914\n",
      "00095: train loss: 0.859, acc: 1.000   valid loss: 0.976, acc: 0.913\n",
      "00096: train loss: 0.856, acc: 1.000   valid loss: 0.972, acc: 0.913\n",
      "00097: train loss: 0.854, acc: 1.000   valid loss: 0.968, acc: 0.913\n",
      "00098: train loss: 0.851, acc: 1.000   valid loss: 0.964, acc: 0.913\n",
      "00099: train loss: 0.849, acc: 1.000   valid loss: 0.960, acc: 0.913\n",
      "00100: train loss: 0.847, acc: 1.000   valid loss: 0.957, acc: 0.913\n",
      "00101: train loss: 0.845, acc: 1.000   valid loss: 0.953, acc: 0.913\n",
      "00102: train loss: 0.842, acc: 1.000   valid loss: 0.950, acc: 0.913\n",
      "00103: train loss: 0.840, acc: 1.000   valid loss: 0.947, acc: 0.913\n",
      "00104: train loss: 0.838, acc: 1.000   valid loss: 0.944, acc: 0.913\n",
      "00105: train loss: 0.836, acc: 1.000   valid loss: 0.941, acc: 0.913\n",
      "00106: train loss: 0.835, acc: 1.000   valid loss: 0.939, acc: 0.913\n",
      "00107: train loss: 0.833, acc: 1.000   valid loss: 0.936, acc: 0.913\n",
      "00108: train loss: 0.831, acc: 1.000   valid loss: 0.934, acc: 0.913\n",
      "00109: train loss: 0.829, acc: 1.000   valid loss: 0.932, acc: 0.913\n",
      "00110: train loss: 0.828, acc: 1.000   valid loss: 0.930, acc: 0.913\n",
      "00111: train loss: 0.826, acc: 1.000   valid loss: 0.928, acc: 0.913\n",
      "00112: train loss: 0.825, acc: 1.000   valid loss: 0.926, acc: 0.913\n",
      "00113: train loss: 0.824, acc: 1.000   valid loss: 0.924, acc: 0.913\n",
      "00114: train loss: 0.823, acc: 1.000   valid loss: 0.923, acc: 0.913\n",
      "00115: train loss: 0.821, acc: 1.000   valid loss: 0.921, acc: 0.913\n",
      "00116: train loss: 0.820, acc: 1.000   valid loss: 0.920, acc: 0.913\n",
      "00117: train loss: 0.819, acc: 1.000   valid loss: 0.918, acc: 0.913\n",
      "00118: train loss: 0.818, acc: 1.000   valid loss: 0.917, acc: 0.913\n",
      "00119: train loss: 0.817, acc: 1.000   valid loss: 0.916, acc: 0.913\n",
      "00120: train loss: 0.816, acc: 1.000   valid loss: 0.915, acc: 0.913\n",
      "00121: train loss: 0.816, acc: 1.000   valid loss: 0.914, acc: 0.913\n",
      "00122: train loss: 0.815, acc: 1.000   valid loss: 0.913, acc: 0.913\n",
      "00123: train loss: 0.814, acc: 1.000   valid loss: 0.912, acc: 0.913\n",
      "00124: train loss: 0.813, acc: 1.000   valid loss: 0.911, acc: 0.913\n",
      "00125: train loss: 0.813, acc: 1.000   valid loss: 0.910, acc: 0.913\n",
      "00126: train loss: 0.812, acc: 1.000   valid loss: 0.910, acc: 0.913\n",
      "00127: train loss: 0.812, acc: 1.000   valid loss: 0.909, acc: 0.913\n",
      "00128: train loss: 0.811, acc: 1.000   valid loss: 0.908, acc: 0.913\n",
      "00129: train loss: 0.811, acc: 1.000   valid loss: 0.908, acc: 0.913\n",
      "00130: train loss: 0.810, acc: 1.000   valid loss: 0.907, acc: 0.913\n",
      "00131: train loss: 0.810, acc: 1.000   valid loss: 0.907, acc: 0.913\n",
      "00132: train loss: 0.810, acc: 1.000   valid loss: 0.907, acc: 0.913\n",
      "00133: train loss: 0.809, acc: 1.000   valid loss: 0.906, acc: 0.913\n",
      "00134: train loss: 0.809, acc: 1.000   valid loss: 0.906, acc: 0.913\n",
      "00135: train loss: 0.809, acc: 1.000   valid loss: 0.906, acc: 0.913\n",
      "00136: train loss: 0.809, acc: 1.000   valid loss: 0.905, acc: 0.913\n",
      "00137: train loss: 0.808, acc: 1.000   valid loss: 0.905, acc: 0.913\n",
      "00138: train loss: 0.808, acc: 1.000   valid loss: 0.905, acc: 0.913\n",
      "00139: train loss: 0.808, acc: 1.000   valid loss: 0.905, acc: 0.913\n",
      "00140: train loss: 0.808, acc: 1.000   valid loss: 0.905, acc: 0.913\n",
      "00141: train loss: 0.808, acc: 1.000   valid loss: 0.905, acc: 0.913\n",
      "00142: train loss: 0.808, acc: 1.000   valid loss: 0.905, acc: 0.913\n",
      "00143: train loss: 0.808, acc: 1.000   valid loss: 0.905, acc: 0.913\n",
      "00144: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00145: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00146: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00147: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00148: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00149: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00150: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00151: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00152: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00153: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00154: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00155: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00156: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00157: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00158: train loss: 0.808, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00159: train loss: 0.807, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00160: train loss: 0.807, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00161: train loss: 0.807, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00162: train loss: 0.807, acc: 1.000   valid loss: 0.904, acc: 0.913\n",
      "00163: train loss: 0.807, acc: 1.000   valid loss: 0.903, acc: 0.913\n",
      "00164: train loss: 0.807, acc: 1.000   valid loss: 0.903, acc: 0.913\n",
      "00165: train loss: 0.807, acc: 1.000   valid loss: 0.903, acc: 0.913\n",
      "00166: train loss: 0.807, acc: 1.000   valid loss: 0.903, acc: 0.913\n",
      "00167: train loss: 0.806, acc: 1.000   valid loss: 0.902, acc: 0.913\n",
      "00168: train loss: 0.806, acc: 1.000   valid loss: 0.902, acc: 0.913\n",
      "00169: train loss: 0.806, acc: 1.000   valid loss: 0.902, acc: 0.913\n",
      "00170: train loss: 0.805, acc: 1.000   valid loss: 0.901, acc: 0.913\n",
      "00171: train loss: 0.805, acc: 1.000   valid loss: 0.901, acc: 0.913\n",
      "00172: train loss: 0.805, acc: 1.000   valid loss: 0.900, acc: 0.913\n",
      "00173: train loss: 0.804, acc: 1.000   valid loss: 0.900, acc: 0.913\n",
      "00174: train loss: 0.804, acc: 1.000   valid loss: 0.899, acc: 0.913\n",
      "00175: train loss: 0.804, acc: 1.000   valid loss: 0.899, acc: 0.913\n",
      "00176: train loss: 0.803, acc: 1.000   valid loss: 0.898, acc: 0.913\n",
      "00177: train loss: 0.803, acc: 1.000   valid loss: 0.897, acc: 0.913\n",
      "00178: train loss: 0.802, acc: 1.000   valid loss: 0.897, acc: 0.913\n",
      "00179: train loss: 0.802, acc: 1.000   valid loss: 0.896, acc: 0.913\n",
      "00180: train loss: 0.801, acc: 1.000   valid loss: 0.895, acc: 0.913\n",
      "00181: train loss: 0.801, acc: 1.000   valid loss: 0.895, acc: 0.913\n",
      "00182: train loss: 0.800, acc: 1.000   valid loss: 0.894, acc: 0.913\n",
      "00183: train loss: 0.799, acc: 1.000   valid loss: 0.893, acc: 0.913\n",
      "00184: train loss: 0.799, acc: 1.000   valid loss: 0.892, acc: 0.913\n",
      "00185: train loss: 0.798, acc: 1.000   valid loss: 0.892, acc: 0.913\n",
      "00186: train loss: 0.797, acc: 1.000   valid loss: 0.891, acc: 0.913\n",
      "00187: train loss: 0.797, acc: 1.000   valid loss: 0.890, acc: 0.913\n",
      "00188: train loss: 0.796, acc: 1.000   valid loss: 0.889, acc: 0.913\n",
      "00189: train loss: 0.795, acc: 1.000   valid loss: 0.888, acc: 0.913\n",
      "00190: train loss: 0.795, acc: 1.000   valid loss: 0.887, acc: 0.913\n",
      "00191: train loss: 0.794, acc: 1.000   valid loss: 0.887, acc: 0.913\n",
      "00192: train loss: 0.793, acc: 1.000   valid loss: 0.886, acc: 0.913\n",
      "00193: train loss: 0.793, acc: 1.000   valid loss: 0.885, acc: 0.913\n",
      "00194: train loss: 0.792, acc: 1.000   valid loss: 0.884, acc: 0.913\n",
      "00195: train loss: 0.791, acc: 1.000   valid loss: 0.883, acc: 0.913\n",
      "00196: train loss: 0.791, acc: 1.000   valid loss: 0.882, acc: 0.913\n",
      "00197: train loss: 0.790, acc: 1.000   valid loss: 0.882, acc: 0.913\n",
      "00198: train loss: 0.789, acc: 1.000   valid loss: 0.881, acc: 0.913\n",
      "00199: train loss: 0.789, acc: 1.000   valid loss: 0.880, acc: 0.914\n",
      "00200: train loss: 0.788, acc: 1.000   valid loss: 0.879, acc: 0.914\n",
      "00201: train loss: 0.788, acc: 1.000   valid loss: 0.879, acc: 0.914\n",
      "00202: train loss: 0.787, acc: 1.000   valid loss: 0.878, acc: 0.914\n",
      "00203: train loss: 0.786, acc: 1.000   valid loss: 0.877, acc: 0.914\n",
      "00204: train loss: 0.786, acc: 1.000   valid loss: 0.877, acc: 0.914\n",
      "00205: train loss: 0.785, acc: 1.000   valid loss: 0.876, acc: 0.914\n",
      "00206: train loss: 0.785, acc: 1.000   valid loss: 0.875, acc: 0.914\n",
      "00207: train loss: 0.784, acc: 1.000   valid loss: 0.875, acc: 0.914\n",
      "00208: train loss: 0.784, acc: 1.000   valid loss: 0.874, acc: 0.914\n",
      "00209: train loss: 0.783, acc: 1.000   valid loss: 0.873, acc: 0.914\n",
      "00210: train loss: 0.783, acc: 1.000   valid loss: 0.873, acc: 0.914\n",
      "00211: train loss: 0.782, acc: 1.000   valid loss: 0.872, acc: 0.914\n",
      "00212: train loss: 0.782, acc: 1.000   valid loss: 0.872, acc: 0.914\n",
      "00213: train loss: 0.781, acc: 1.000   valid loss: 0.871, acc: 0.914\n",
      "00214: train loss: 0.781, acc: 1.000   valid loss: 0.871, acc: 0.914\n",
      "00215: train loss: 0.781, acc: 1.000   valid loss: 0.871, acc: 0.914\n",
      "00216: train loss: 0.780, acc: 1.000   valid loss: 0.870, acc: 0.914\n",
      "00217: train loss: 0.780, acc: 1.000   valid loss: 0.870, acc: 0.914\n",
      "00218: train loss: 0.780, acc: 1.000   valid loss: 0.869, acc: 0.914\n",
      "00219: train loss: 0.779, acc: 1.000   valid loss: 0.869, acc: 0.914\n",
      "00220: train loss: 0.779, acc: 1.000   valid loss: 0.869, acc: 0.914\n",
      "00221: train loss: 0.779, acc: 1.000   valid loss: 0.868, acc: 0.914\n",
      "00222: train loss: 0.779, acc: 1.000   valid loss: 0.868, acc: 0.914\n",
      "00223: train loss: 0.778, acc: 1.000   valid loss: 0.868, acc: 0.914\n",
      "00224: train loss: 0.778, acc: 1.000   valid loss: 0.868, acc: 0.914\n",
      "00225: train loss: 0.778, acc: 1.000   valid loss: 0.867, acc: 0.914\n",
      "00226: train loss: 0.778, acc: 1.000   valid loss: 0.867, acc: 0.914\n",
      "00227: train loss: 0.778, acc: 1.000   valid loss: 0.867, acc: 0.914\n",
      "00228: train loss: 0.777, acc: 1.000   valid loss: 0.867, acc: 0.914\n",
      "00229: train loss: 0.777, acc: 1.000   valid loss: 0.867, acc: 0.914\n",
      "00230: train loss: 0.777, acc: 1.000   valid loss: 0.867, acc: 0.914\n",
      "00231: train loss: 0.777, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00232: train loss: 0.777, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00233: train loss: 0.777, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00234: train loss: 0.777, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00235: train loss: 0.777, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00236: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00237: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00238: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00239: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00240: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00241: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00242: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00243: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00244: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00245: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00246: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00247: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00248: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00249: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00250: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00251: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00252: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00253: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00254: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00255: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00256: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00257: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00258: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00259: train loss: 0.776, acc: 1.000   valid loss: 0.866, acc: 0.914\n",
      "00260: train loss: 0.776, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00261: train loss: 0.776, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00262: train loss: 0.776, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00263: train loss: 0.776, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00264: train loss: 0.776, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00265: train loss: 0.776, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00266: train loss: 0.776, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00267: train loss: 0.776, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00268: train loss: 0.776, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00269: train loss: 0.775, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00270: train loss: 0.775, acc: 1.000   valid loss: 0.865, acc: 0.914\n",
      "00271: train loss: 0.775, acc: 1.000   valid loss: 0.864, acc: 0.914\n",
      "00272: train loss: 0.775, acc: 1.000   valid loss: 0.864, acc: 0.914\n",
      "00273: train loss: 0.775, acc: 1.000   valid loss: 0.864, acc: 0.914\n",
      "00274: train loss: 0.775, acc: 1.000   valid loss: 0.864, acc: 0.914\n",
      "00275: train loss: 0.775, acc: 1.000   valid loss: 0.864, acc: 0.914\n",
      "00276: train loss: 0.774, acc: 1.000   valid loss: 0.864, acc: 0.914\n",
      "00277: train loss: 0.774, acc: 1.000   valid loss: 0.863, acc: 0.914\n",
      "00278: train loss: 0.774, acc: 1.000   valid loss: 0.863, acc: 0.914\n",
      "00279: train loss: 0.774, acc: 1.000   valid loss: 0.863, acc: 0.914\n",
      "00280: train loss: 0.774, acc: 1.000   valid loss: 0.863, acc: 0.914\n",
      "00281: train loss: 0.774, acc: 1.000   valid loss: 0.862, acc: 0.914\n",
      "00282: train loss: 0.773, acc: 1.000   valid loss: 0.862, acc: 0.914\n",
      "00283: train loss: 0.773, acc: 1.000   valid loss: 0.862, acc: 0.914\n",
      "00284: train loss: 0.773, acc: 1.000   valid loss: 0.862, acc: 0.914\n",
      "00285: train loss: 0.773, acc: 1.000   valid loss: 0.861, acc: 0.914\n",
      "00286: train loss: 0.772, acc: 1.000   valid loss: 0.861, acc: 0.914\n",
      "00287: train loss: 0.772, acc: 1.000   valid loss: 0.861, acc: 0.914\n",
      "00288: train loss: 0.772, acc: 1.000   valid loss: 0.860, acc: 0.914\n",
      "00289: train loss: 0.772, acc: 1.000   valid loss: 0.860, acc: 0.914\n",
      "00290: train loss: 0.771, acc: 1.000   valid loss: 0.860, acc: 0.914\n",
      "00291: train loss: 0.771, acc: 1.000   valid loss: 0.860, acc: 0.914\n",
      "00292: train loss: 0.771, acc: 1.000   valid loss: 0.859, acc: 0.914\n",
      "00293: train loss: 0.771, acc: 1.000   valid loss: 0.859, acc: 0.914\n",
      "00294: train loss: 0.770, acc: 1.000   valid loss: 0.859, acc: 0.914\n",
      "00295: train loss: 0.770, acc: 1.000   valid loss: 0.858, acc: 0.914\n",
      "00296: train loss: 0.770, acc: 1.000   valid loss: 0.858, acc: 0.914\n",
      "00297: train loss: 0.770, acc: 1.000   valid loss: 0.858, acc: 0.914\n",
      "00298: train loss: 0.769, acc: 1.000   valid loss: 0.858, acc: 0.914\n",
      "00299: train loss: 0.769, acc: 1.000   valid loss: 0.857, acc: 0.914\n",
      "00300: train loss: 0.769, acc: 1.000   valid loss: 0.857, acc: 0.914\n",
      "00301: train loss: 0.769, acc: 1.000   valid loss: 0.857, acc: 0.914\n",
      "00302: train loss: 0.768, acc: 1.000   valid loss: 0.856, acc: 0.914\n",
      "00303: train loss: 0.768, acc: 1.000   valid loss: 0.856, acc: 0.914\n",
      "00304: train loss: 0.768, acc: 1.000   valid loss: 0.856, acc: 0.914\n",
      "00305: train loss: 0.768, acc: 1.000   valid loss: 0.856, acc: 0.914\n",
      "00306: train loss: 0.767, acc: 1.000   valid loss: 0.855, acc: 0.914\n",
      "00307: train loss: 0.767, acc: 1.000   valid loss: 0.855, acc: 0.914\n",
      "00308: train loss: 0.767, acc: 1.000   valid loss: 0.855, acc: 0.914\n",
      "00309: train loss: 0.767, acc: 1.000   valid loss: 0.855, acc: 0.914\n",
      "00310: train loss: 0.767, acc: 1.000   valid loss: 0.855, acc: 0.914\n",
      "00311: train loss: 0.766, acc: 1.000   valid loss: 0.854, acc: 0.914\n",
      "00312: train loss: 0.766, acc: 1.000   valid loss: 0.854, acc: 0.914\n",
      "00313: train loss: 0.766, acc: 1.000   valid loss: 0.854, acc: 0.914\n",
      "00314: train loss: 0.766, acc: 1.000   valid loss: 0.854, acc: 0.914\n",
      "00315: train loss: 0.766, acc: 1.000   valid loss: 0.854, acc: 0.914\n",
      "00316: train loss: 0.766, acc: 1.000   valid loss: 0.853, acc: 0.914\n",
      "00317: train loss: 0.766, acc: 1.000   valid loss: 0.853, acc: 0.914\n",
      "00318: train loss: 0.765, acc: 1.000   valid loss: 0.853, acc: 0.914\n",
      "00319: train loss: 0.765, acc: 1.000   valid loss: 0.853, acc: 0.914\n",
      "00320: train loss: 0.765, acc: 1.000   valid loss: 0.853, acc: 0.914\n",
      "00321: train loss: 0.765, acc: 1.000   valid loss: 0.853, acc: 0.914\n",
      "00322: train loss: 0.765, acc: 1.000   valid loss: 0.853, acc: 0.914\n",
      "00323: train loss: 0.765, acc: 1.000   valid loss: 0.853, acc: 0.914\n",
      "00324: train loss: 0.765, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00325: train loss: 0.765, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00326: train loss: 0.765, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00327: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00328: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00329: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00330: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00331: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00332: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00333: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00334: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00335: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00336: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00337: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00338: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00339: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00340: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00341: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00342: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00343: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00344: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00345: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00346: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00347: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00348: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00349: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00350: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00351: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00352: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00353: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00354: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00355: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00356: train loss: 0.764, acc: 1.000   valid loss: 0.852, acc: 0.914\n",
      "00357: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00358: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00359: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00360: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00361: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00362: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00363: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00364: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00365: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00366: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00367: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00368: train loss: 0.764, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00369: train loss: 0.763, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00370: train loss: 0.763, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00371: train loss: 0.763, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00372: train loss: 0.763, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00373: train loss: 0.763, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00374: train loss: 0.763, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00375: train loss: 0.763, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00376: train loss: 0.763, acc: 1.000   valid loss: 0.851, acc: 0.914\n",
      "00377: train loss: 0.763, acc: 1.000   valid loss: 0.850, acc: 0.914\n",
      "00378: train loss: 0.763, acc: 1.000   valid loss: 0.850, acc: 0.914\n",
      "00379: train loss: 0.763, acc: 1.000   valid loss: 0.850, acc: 0.914\n",
      "00380: train loss: 0.763, acc: 1.000   valid loss: 0.850, acc: 0.914\n",
      "00381: train loss: 0.763, acc: 1.000   valid loss: 0.850, acc: 0.914\n",
      "00382: train loss: 0.763, acc: 1.000   valid loss: 0.850, acc: 0.914\n",
      "00383: train loss: 0.762, acc: 1.000   valid loss: 0.850, acc: 0.914\n",
      "00384: train loss: 0.762, acc: 1.000   valid loss: 0.850, acc: 0.914\n",
      "00385: train loss: 0.762, acc: 1.000   valid loss: 0.850, acc: 0.914\n",
      "00386: train loss: 0.762, acc: 1.000   valid loss: 0.849, acc: 0.914\n",
      "00387: train loss: 0.762, acc: 1.000   valid loss: 0.849, acc: 0.914\n",
      "00388: train loss: 0.762, acc: 1.000   valid loss: 0.849, acc: 0.914\n",
      "00389: train loss: 0.762, acc: 1.000   valid loss: 0.849, acc: 0.914\n",
      "00390: train loss: 0.762, acc: 1.000   valid loss: 0.849, acc: 0.914\n",
      "00391: train loss: 0.761, acc: 1.000   valid loss: 0.849, acc: 0.914\n",
      "00392: train loss: 0.761, acc: 1.000   valid loss: 0.849, acc: 0.914\n",
      "00393: train loss: 0.761, acc: 1.000   valid loss: 0.848, acc: 0.914\n",
      "00394: train loss: 0.761, acc: 1.000   valid loss: 0.848, acc: 0.914\n",
      "00395: train loss: 0.761, acc: 1.000   valid loss: 0.848, acc: 0.914\n",
      "00396: train loss: 0.761, acc: 1.000   valid loss: 0.848, acc: 0.914\n",
      "00397: train loss: 0.761, acc: 1.000   valid loss: 0.848, acc: 0.914\n",
      "00398: train loss: 0.761, acc: 1.000   valid loss: 0.848, acc: 0.914\n",
      "00399: train loss: 0.760, acc: 1.000   valid loss: 0.848, acc: 0.914\n",
      "00400: train loss: 0.760, acc: 1.000   valid loss: 0.847, acc: 0.914\n",
      "00401: train loss: 0.760, acc: 1.000   valid loss: 0.847, acc: 0.914\n",
      "00402: train loss: 0.760, acc: 1.000   valid loss: 0.847, acc: 0.914\n",
      "00403: train loss: 0.760, acc: 1.000   valid loss: 0.847, acc: 0.914\n",
      "00404: train loss: 0.760, acc: 1.000   valid loss: 0.847, acc: 0.914\n",
      "00405: train loss: 0.760, acc: 1.000   valid loss: 0.847, acc: 0.914\n",
      "00406: train loss: 0.760, acc: 1.000   valid loss: 0.847, acc: 0.914\n",
      "00407: train loss: 0.760, acc: 1.000   valid loss: 0.847, acc: 0.914\n",
      "00408: train loss: 0.759, acc: 1.000   valid loss: 0.846, acc: 0.914\n",
      "00409: train loss: 0.759, acc: 1.000   valid loss: 0.846, acc: 0.914\n",
      "00410: train loss: 0.759, acc: 1.000   valid loss: 0.846, acc: 0.914\n",
      "00411: train loss: 0.759, acc: 1.000   valid loss: 0.846, acc: 0.914\n",
      "00412: train loss: 0.759, acc: 1.000   valid loss: 0.846, acc: 0.914\n",
      "00413: train loss: 0.759, acc: 1.000   valid loss: 0.846, acc: 0.914\n",
      "00414: train loss: 0.759, acc: 1.000   valid loss: 0.846, acc: 0.914\n",
      "00415: train loss: 0.759, acc: 1.000   valid loss: 0.846, acc: 0.914\n",
      "00416: train loss: 0.759, acc: 1.000   valid loss: 0.846, acc: 0.914\n",
      "00417: train loss: 0.759, acc: 1.000   valid loss: 0.846, acc: 0.914\n",
      "00418: train loss: 0.759, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00419: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00420: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00421: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00422: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00423: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00424: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00425: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00426: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00427: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00428: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00429: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00430: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00431: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00432: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00433: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00434: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00435: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00436: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00437: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00438: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00439: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00440: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00441: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00442: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00443: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00444: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00445: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00446: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00447: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00448: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00449: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00450: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00451: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00452: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00453: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00454: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00455: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00456: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00457: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00458: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00459: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00460: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00461: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00462: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00463: train loss: 0.758, acc: 1.000   valid loss: 0.845, acc: 0.914\n",
      "00464: train loss: 0.758, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00465: train loss: 0.758, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00466: train loss: 0.758, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00467: train loss: 0.758, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00468: train loss: 0.758, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00469: train loss: 0.758, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00470: train loss: 0.758, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00471: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00472: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00473: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00474: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00475: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00476: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00477: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00478: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00479: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00480: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00481: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00482: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00483: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00484: train loss: 0.757, acc: 1.000   valid loss: 0.844, acc: 0.914\n",
      "00485: train loss: 0.757, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00486: train loss: 0.757, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00487: train loss: 0.757, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00488: train loss: 0.757, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00489: train loss: 0.757, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00490: train loss: 0.756, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00491: train loss: 0.756, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00492: train loss: 0.756, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00493: train loss: 0.756, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00494: train loss: 0.756, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00495: train loss: 0.756, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00496: train loss: 0.756, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00497: train loss: 0.756, acc: 1.000   valid loss: 0.843, acc: 0.914\n",
      "00498: train loss: 0.756, acc: 1.000   valid loss: 0.842, acc: 0.914\n",
      "00499: train loss: 0.756, acc: 1.000   valid loss: 0.842, acc: 0.914\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer)\n",
    "    lr_scheduler.step()\n",
    "    valid_loss, valid_acc = evalute(model, valid_loader)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ep = str(epoch).zfill(5)\n",
    "\n",
    "    print(f'{ep}: train loss: {train_loss:2.3f}, acc: {train_acc:2.3f}   valid loss: {valid_loss:2.3f}, acc: {valid_acc:2.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23719eaf-64e5-4011-b6e3-99c822ddf8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_eval(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (sequences, labels) in enumerate(dataloader):\n",
    "            \n",
    "            sequences = sequences.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            one_hot = F.one_hot(labels, num_classes=num_of_classes).to(device)\n",
    "            one_hot = one_hot[:, None, :].repeat((1, 5, 1)).float()\n",
    "\n",
    "            #############\n",
    "            # generator #\n",
    "            #############\n",
    "            predict_probability = model(sequences[:, :, :, :9])\n",
    "\n",
    "            predict_probability = predict_probability.reshape((-1, num_of_classes))\n",
    "            one_hot = one_hot.reshape((-1, num_of_classes))\n",
    "            _, predict_classes = torch.max(predict_probability, 1)\n",
    "            predict_classes = predict_classes.cpu().detach().numpy()\n",
    "            _, labels = torch.max(one_hot, 1)\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "            \n",
    "            print(f'{i: >3} predict class: {predict_classes}')\n",
    "            print(f'{i: >3}  ground truth: {labels}')\n",
    "\n",
    "            loss = ce_loss(predict_probability, one_hot)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(accuracy_score(labels, predict_classes))\n",
    "            \n",
    "    print(f'loss: {np.mean(losses): 2.3f}, accuracy: {np.mean(accuracies): 2.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f1c0beb-8280-4a77-a23a-e421405bb981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  0  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  1 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  1  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  2 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  2  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  3 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  3  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  4 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  4  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  5 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  5  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  6 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  6  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  7 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  7  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  8 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  8  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  9 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "  9  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " 10 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " 10  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " 11 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " 11  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " 12 predict class: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " 12  ground truth: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " 13 predict class: [1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 13  ground truth: [1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 14 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 14  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 15 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 15  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 16 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 16  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 17 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 17  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 18 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 18  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 19 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 19  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 20 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 20  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 21 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 21  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 22 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 22  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 23 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 23  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 24 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 24  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 25 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 25  ground truth: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 26 predict class: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 26  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 27 predict class: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 27  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 28 predict class: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 28  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 29 predict class: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 29  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 30 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 30  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 31 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 31  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 32 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 32  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 33 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 33  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 34 predict class: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 34  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 35 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 35  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 36 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " 36  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " 37 predict class: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 37  ground truth: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 38 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 38  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 39 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 39  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 40 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 40  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 41 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 41  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 42 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 42  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 43 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 43  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 44 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 44  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 45 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 45  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 46 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 46  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 47 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 47  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 48 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 48  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 49 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 49  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 50 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 50  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 51 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 51  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 52 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 52  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 53 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 53  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 54 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 54  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 55 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 55  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 56 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 56  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 57 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 57  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 58 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 58  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 59 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 59  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 60 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 60  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 61 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 61  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 62 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 62  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 63 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 63  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 64 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 64  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 65 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 65  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 66 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 66  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 67 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 67  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 68 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 68  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 69 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 69  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 70 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 70  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 71 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 71  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 72 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 72  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 73 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 73  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 74 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 74  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " 75 predict class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      " 75  ground truth: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "loss:  0.832, accuracy:  0.914\n"
     ]
    }
   ],
   "source": [
    "output_eval(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1bea9-59c8-44c6-9423-078f3cefe4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fab0c5-1510-4d9b-b7d9-349b2d245767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mag39",
   "language": "python",
   "name": "mag39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
