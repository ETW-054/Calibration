{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a92978-52d9-4cc8-b309-2a8874549ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c01a44-c290-4f08-8df4-97acc58764bf",
   "metadata": {},
   "source": [
    "# 解壓縮資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34a0f5a-04f9-4bfb-9154-2001904235c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_data(path):\n",
    "    for folder, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('zip'):\n",
    "                file_path = os.path.join(folder, file)\n",
    "                print(file_path)\n",
    "\n",
    "                sotre_path = os.path.join(folder, file.rsplit('.')[0])\n",
    "                # 開啟 ZIP 壓縮檔 \n",
    "                with zipfile.ZipFile(file_path, 'r') as zf:\n",
    "                    # 解壓縮所有檔案至 /my/folder 目錄\n",
    "                    zf.extractall(path=sotre_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cecf48e8-21b8-46cd-a24d-6d79e9857780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip_data('./swing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986b396b-062a-4ebb-b6a1-1de47dbbb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv(path):\n",
    "    acc_df = pd.read_csv(os.path.join(path, 'Accelerometer.csv'), delimiter=',')\n",
    "    gyo_df = pd.read_csv(os.path.join(path, 'Gyroscope.csv'), delimiter=',')\n",
    "    linacc_df = pd.read_csv(os.path.join(path, 'Linear Accelerometer.csv'), delimiter=',')\n",
    "    mag_df = pd.read_csv(os.path.join(path, 'Magnetometer.csv'), delimiter=',')\n",
    "    device_df = pd.read_csv(os.path.join(path, 'meta', 'device.csv'), delimiter=',')\n",
    "    time_df = pd.read_csv(os.path.join(path, 'meta', 'time.csv'), delimiter=',')\n",
    "    \n",
    "    acc_df.to_csv(os.path.join(path, 'Accelerometer.csv'), index=False, sep=';')\n",
    "    gyo_df.to_csv(os.path.join(path, 'Gyroscope.csv'), index=False, sep=';')\n",
    "    linacc_df.to_csv(os.path.join(path, 'Linear Accelerometer.csv'), index=False, sep=';')\n",
    "    mag_df.to_csv(os.path.join(path, 'Magnetometer.csv'), index=False, sep=';')\n",
    "    device_df.to_csv(os.path.join(path, 'meta', 'device.csv'), index=False, sep=';')\n",
    "    time_df.to_csv(os.path.join(path, 'meta', 'time.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a400be-465b-46d6-8920-99424db47df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_csv('./pocket/202301101952/target')\n",
    "# convert_csv('./pocket/202301101952/source')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d0f88-db27-4381-bfa6-8a7f86930454",
   "metadata": {},
   "source": [
    "# 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b5f5c14-bd7c-4fb4-99c0-0a0cb769baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_data(df):\n",
    "    new_names = ['system_time', 'acc_times', 'acc_x', 'acc_y', 'acc_z', 'gyo_times', 'gyo_x', 'gyo_y', 'gyo_z', 'lin_acc_times', 'lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'mag_times', 'mag_x', 'mag_y', 'mag_z']\n",
    "    df.columns = new_names\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def device_start_system_time(path):\n",
    "    time_df = pd.read_csv(path, delimiter=';', index_col=0)\n",
    "    time = time_df.T.loc['system time', 'START']\n",
    "    \n",
    "    return time\n",
    "\n",
    "\n",
    "def load_original_data(path):\n",
    "    acc_df = pd.read_csv(os.path.join(path, 'Accelerometer.csv'), delimiter=';')\n",
    "    gyo_df = pd.read_csv(os.path.join(path, 'Gyroscope.csv'), delimiter=';')\n",
    "    linacc_df = pd.read_csv(os.path.join(path, 'Linear Accelerometer.csv'), delimiter=';')\n",
    "    mag_df = pd.read_csv(os.path.join(path, 'Magnetometer.csv'), delimiter=';')\n",
    "    start_time = device_start_system_time(os.path.join(path, 'meta/time.csv'))\n",
    "    time_df = acc_df.iloc[:, 0] + start_time\n",
    "    \n",
    "    total_df = pd.concat([time_df, acc_df, gyo_df, linacc_df, mag_df], axis=1)\n",
    "    total_df = rename_data(total_df)\n",
    "    \n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50962f15-f38a-429a-b00a-2adf9a6eb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_data(source_df, target_df):\n",
    "    source_start_time = source_df.loc[0, 'system_time']\n",
    "    target_start_time = target_df.loc[0, 'system_time']\n",
    "    \n",
    "    # align start time\n",
    "    if source_start_time > target_start_time:  # source start time > target start time\n",
    "        target_start_idx = np.argmin(np.abs(target_df.system_time - source_start_time))\n",
    "        target_df = target_df.iloc[target_start_idx:].reset_index(drop=True)\n",
    "    else:  # source start time < target start time\n",
    "        source_start_idx = np.argmin(np.abs(source_df.system_time - target_start_time))\n",
    "        source_df = source_df.iloc[source_start_idx:].reset_index(drop=True)\n",
    "        \n",
    "    # align end idx\n",
    "    end_idx = min(len(source_df), len(target_df))\n",
    "    source_df = source_df.iloc[:end_idx]\n",
    "    target_df = target_df.iloc[:end_idx]\n",
    "    \n",
    "    return source_df, target_df\n",
    "\n",
    "\n",
    "def bound_range(df):\n",
    "    start = datapoint_per_second * 35\n",
    "    end = len(df) - datapoint_per_second * 20\n",
    "    \n",
    "    return df.iloc[start:end].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def split_segments(df, duration=5):\n",
    "    length = datapoint_per_second * duration\n",
    "    num_of_segs = int(np.floor(len(df) / length))\n",
    "    \n",
    "    segments = []\n",
    "    for i in range(num_of_segs):\n",
    "        segments.append(df.iloc[int(i * length):int((i + 1) * length)].to_numpy())\n",
    "        \n",
    "    return segments\n",
    "\n",
    "\n",
    "def select_data(df):\n",
    "    return df[['gyo_x', 'gyo_y', 'gyo_z', 'lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'mag_x', 'mag_y', 'mag_z', 'system_time']]\n",
    "\n",
    "\n",
    "def preprocess_data(df, duration):\n",
    "    pre_df = select_data(df)\n",
    "    segs = split_segments(pre_df, duration)\n",
    "    \n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e99a24c-4c63-4e78-ae26-446adf71d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = load_original_data('./front_pocket/202302071724/source')\n",
    "# segs = preprocess_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b3841d1-a447-429f-b14a-7e3d70581d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_df = load_original_data('./front_pocket/202302071704/source')\n",
    "# target_df = load_original_data('./front_pocket/202302071704/target')\n",
    "# print(source_df.system_time[0], target_df.system_time[0])\n",
    "# sdf, tdf = align_data(source_df, target_df)\n",
    "# print(sdf.system_time[0], tdf.system_time[0])\n",
    "# print(sdf.system_time[len(sdf) - 1], tdf.system_time[len(tdf) - 1])\n",
    "# print(len(sdf), len(tdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c57b7b75-a898-49d3-8b69-a7ac8fe74114",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint_per_second = 20\n",
    "duration = 2\n",
    "classes = {'target': 0, 'front_pocket': 1, 'pocket': 2, 'swing': 3}\n",
    "\n",
    "def device_version(path):\n",
    "    device_df = pd.read_csv(path, delimiter=';', index_col=0)\n",
    "    version = device_df.loc['deviceRelease'].value\n",
    "    \n",
    "    return version\n",
    "\n",
    "\n",
    "def check_data_device(source_path, target_path):\n",
    "    while True:\n",
    "        source_version = device_version(os.path.join(source_path, 'meta/device.csv'))\n",
    "        target_version = device_version(os.path.join(target_path, 'meta/device.csv'))\n",
    "\n",
    "        print(source_path, target_path)\n",
    "\n",
    "        if source_version[:2] == '15' and target_version[:2] == '16':\n",
    "            return source_path, target_path\n",
    "        elif source_version[:2] == '16' and target_version[:2] == '15':\n",
    "            source_path = os.path.join(folder_path, 'target')\n",
    "            target_path = os.path.join(folder_path, 'source')\n",
    "            print('--- GG ---')\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def load_pair_data(root_folder, class_num):\n",
    "    pair_data = []\n",
    "\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        source_path = os.path.join(folder_path, 'source')\n",
    "        target_path = os.path.join(folder_path, 'target')\n",
    "        \n",
    "        print(folder_path)\n",
    "        \n",
    "        #########################\n",
    "        ##### check devices #####\n",
    "        #########################\n",
    "        source_path, target_path = check_data_device(source_path, target_path)\n",
    "        \n",
    "        ####################################\n",
    "        ##### load and preprocess data #####\n",
    "        ####################################\n",
    "        source_df = load_original_data(source_path)\n",
    "        target_df = load_original_data(target_path)\n",
    "        \n",
    "#         print(source_df.system_time[0], target_df.system_time[0])\n",
    "        \n",
    "        source_df, target_df = align_data(source_df, target_df)\n",
    "        source_df, target_df = bound_range(source_df), bound_range(target_df)\n",
    "        \n",
    "#         print(source_df.system_time[0], target_df.system_time[0])\n",
    "#         print(source_df.system_time[len(source_df) - 1], target_df.system_time[len(target_df) - 1])\n",
    "#         print(len(source_df), len(target_df))\n",
    "        \n",
    "#         plt.figure(figsize=(30, 5))\n",
    "#         plt.plot(np.arange(len(source_df)), source_df.acc_x)\n",
    "#         plt.plot(np.arange(len(target_df)), target_df.acc_y)\n",
    "#         plt.show()\n",
    "        \n",
    "        source_segs = preprocess_data(source_df, duration)\n",
    "        target_segs = preprocess_data(target_df, duration)\n",
    "        \n",
    "        idx = min(len(source_segs), len(target_segs))\n",
    "        source_tags = [class_num] * idx\n",
    "        target_tags = [0] * idx\n",
    "        \n",
    "        pair_data.extend(zip(source_segs[:idx], source_tags, target_segs[:idx], target_tags))\n",
    "        \n",
    "    return pair_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34aa6ce4-9bb1-428a-b421-0d086fc4cba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15.4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_version('./front_pocket/202302071523/source/meta/device.csv')  # source version: 15.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "096e5ad0-4606-4ef4-aa30-ff27a1d21cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16.3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_version('./front_pocket/202302071523/target/meta/device.csv')  # target version: 16.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee569c5-fda8-44e6-be6b-3aff9efb2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./front_pocket/202302071628\n",
      "./front_pocket/202302071628/source ./front_pocket/202302071628/target\n",
      "./front_pocket/202302071652\n",
      "./front_pocket/202302071652/source ./front_pocket/202302071652/target\n",
      "./front_pocket/202302071523\n",
      "./front_pocket/202302071523/source ./front_pocket/202302071523/target\n",
      "./front_pocket/202302071531\n",
      "./front_pocket/202302071531/source ./front_pocket/202302071531/target\n",
      "./front_pocket/202302071715\n",
      "./front_pocket/202302071715/source ./front_pocket/202302071715/target\n",
      "./front_pocket/202302071641\n",
      "./front_pocket/202302071641/source ./front_pocket/202302071641/target\n",
      "./front_pocket/202302071541\n",
      "./front_pocket/202302071541/source ./front_pocket/202302071541/target\n",
      "./front_pocket/202302071619\n",
      "./front_pocket/202302071619/source ./front_pocket/202302071619/target\n",
      "./front_pocket/202302071704\n",
      "./front_pocket/202302071704/source ./front_pocket/202302071704/target\n",
      "./front_pocket/202302071724\n",
      "./front_pocket/202302071724/source ./front_pocket/202302071724/target\n",
      "./pocket/202302132108\n",
      "./pocket/202302132108/source ./pocket/202302132108/target\n",
      "./pocket/202302131750\n",
      "./pocket/202302131750/source ./pocket/202302131750/target\n",
      "./pocket/202302131601\n",
      "./pocket/202302131601/source ./pocket/202302131601/target\n",
      "./pocket/202302071606\n",
      "./pocket/202302071606/source ./pocket/202302071606/target\n",
      "./pocket/202302132053\n",
      "./pocket/202302132053/source ./pocket/202302132053/target\n",
      "./pocket/202302122132\n",
      "./pocket/202302122132/source ./pocket/202302122132/target\n",
      "./pocket/202302132116\n",
      "./pocket/202302132116/source ./pocket/202302132116/target\n",
      "./pocket/202302131643\n",
      "./pocket/202302131643/source ./pocket/202302131643/target\n",
      "./pocket/202301101952\n",
      "./pocket/202301101952/source ./pocket/202301101952/target\n",
      "./pocket/202302132101\n",
      "./pocket/202302132101/source ./pocket/202302132101/target\n",
      "./swing/202302142339\n",
      "./swing/202302142339/source ./swing/202302142339/target\n",
      "./swing/202302132131\n",
      "./swing/202302132131/source ./swing/202302132131/target\n",
      "./swing/202302142117\n",
      "./swing/202302142117/source ./swing/202302142117/target\n",
      "./swing/202302132124\n",
      "./swing/202302132124/source ./swing/202302132124/target\n",
      "./swing/202302121947\n",
      "./swing/202302121947/source ./swing/202302121947/target\n",
      "./swing/202302121857\n",
      "./swing/202302121857/source ./swing/202302121857/target\n",
      "./swing/202302142128\n",
      "./swing/202302142128/source ./swing/202302142128/target\n",
      "./swing/202302121920\n",
      "./swing/202302121920/source ./swing/202302121920/target\n",
      "./swing/202302121909\n",
      "./swing/202302121909/source ./swing/202302121909/target\n",
      "./swing/202302142331\n",
      "./swing/202302142331/source ./swing/202302142331/target\n"
     ]
    }
   ],
   "source": [
    "front_pocket_pair_data = load_pair_data('./front_pocket', class_num=1)\n",
    "pocket_pair_data = load_pair_data('./pocket', class_num=2)\n",
    "swing_pair_data = load_pair_data('./swing', class_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a5c707-8a25-4a06-874d-df286b5599c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1439 1374 1346\n"
     ]
    }
   ],
   "source": [
    "print(len(front_pocket_pair_data), len(pocket_pair_data), len(swing_pair_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2f40e0-0448-490c-9449-232c6a55bd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1675758113.8518965, 1675758113.7758677)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front_pocket_pair_data[0][0][0, -1], front_pocket_pair_data[0][2][0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c205df6-e866-40ac-a8dc-13b69e39b8df",
   "metadata": {},
   "source": [
    "# 建立dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "073af591-76e1-47ec-a306-252417490eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ada036cd-d67b-4d25-b935-0cdcde61f750",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ClassDataset(Dataset):\n",
    "#     def __init__(self, data, label):\n",
    "#         self.data = data\n",
    "#         self.label = label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx], self.label[idx]\n",
    "\n",
    "\n",
    "# class SimpleRNN(nn.Module):\n",
    "#     def __init__(self, seq_len=100, num_of_classes=2):\n",
    "#         super(SimpleRNN, self).__init__()\n",
    "        \n",
    "#         self.seq_len = seq_len\n",
    "        \n",
    "#         self.layer0 = nn.Sequential(\n",
    "#             nn.Linear(12, 24),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(24, 32),\n",
    "#             nn.LeakyReLU(),\n",
    "#         )\n",
    "        \n",
    "#         self.rnn = nn.RNN(input_size=32, hidden_size=32, num_layers=2, batch_first=True)\n",
    "# #         self.lstm = nn.LSTM(input_size=16, hidden_size=16, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "#         self.last = nn.Sequential(\n",
    "#             nn.Linear(32, 32),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(32, num_of_classes),\n",
    "#             nn.Softmax(dim=1),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         h = self.layer0(x)\n",
    "        \n",
    "#         hz, _ = self.rnn(h)\n",
    "        \n",
    "#         out = self.last(hz[:, -1])\n",
    "        \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c7e70f1-e862-4b7d-9836-4d5eb3b28d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, source_data, source_label, target_data, target_label):\n",
    "        self.source_data = source_data\n",
    "        self.source_label = source_label\n",
    "        self.target_data = target_data\n",
    "        self.target_label = target_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_data[idx], self.source_label[idx], self.target_data[idx], self.target_label[idx]\n",
    "\n",
    "\n",
    "class NotSimpleRNN(nn.Module):\n",
    "    def __init__(self, seq_len=100, num_of_classes=2):\n",
    "        super(NotSimpleRNN, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        ################\n",
    "        ### Imu Part ###\n",
    "        ################\n",
    "        self.imu_layer = nn.Sequential(\n",
    "            nn.Linear(8, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.imu_rnn = nn.LSTM(input_size=32, hidden_size=32, num_layers=2, batch_first=True)\n",
    "#         self.lstm = nn.LSTM(input_size=16, hidden_size=16, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.imu_last = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, num_of_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "        ################\n",
    "        ### Mag Part ###\n",
    "        ################\n",
    "        self.mag_layer = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.mag_rnn_encoder = nn.LSTM(input_size=16, hidden_size=16, num_layers=4, batch_first=True)\n",
    "        self.mag_rnn_decoder = nn.LSTM(input_size=48, hidden_size=16, num_layers=4, batch_first=True)\n",
    "        self.mag_last = nn.Sequential(\n",
    "            nn.Linear(16, 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, 3),\n",
    "        )\n",
    "        \n",
    "    def forward(self, source_imu, source_mag):\n",
    "        ### imu part ###\n",
    "        source_h = self.imu_layer(source_imu)\n",
    "        source_hz, _ = self.imu_rnn(source_h)\n",
    "        source_predict_probability = self.imu_last(source_hz[:, -1])\n",
    "        \n",
    "        ### mag part ###\n",
    "        source_mag_h = self.mag_layer(source_mag)  # (batch, seq_len, 16)\n",
    "        source_mag_hz, _ = self.mag_rnn_encoder(source_mag_h)  # (batch, seq_len, 16)\n",
    "        \n",
    "        source_latent = torch.concat([source_hz, source_mag_hz], dim=-1)  # (batch, seq_len, 48)\n",
    "        \n",
    "        predict_mag_latent, _ = self.mag_rnn_decoder(source_latent)\n",
    "        predict_mag = self.mag_last(predict_mag_latent)\n",
    "        \n",
    "        return source_predict_probability, predict_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a7c7b91-8abf-4ecc-9efd-90916423b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tgt_mask(size) -> torch.tensor:\n",
    "    # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "    mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "    mask = mask.float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "    mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "\n",
    "    # EX for size=5:\n",
    "    # [[0., -inf, -inf, -inf, -inf],\n",
    "    #  [0.,   0., -inf, -inf, -inf],\n",
    "    #  [0.,   0.,   0., -inf, -inf],\n",
    "    #  [0.,   0.,   0.,   0., -inf],\n",
    "    #  [0.,   0.,   0.,   0.,   0.]]\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47e39fde-bc5a-4809-8b48-b4dc6ceb7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotSimpleTransformer(nn.Module):\n",
    "    def __init__(self, seq_len=100, num_of_classes=2):\n",
    "        super(NotSimpleTransformer, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.pos_encoder = PositionalEncoding(64, 0.1)\n",
    "        ################\n",
    "        ### Imu Part ###\n",
    "        ################\n",
    "#         self.imu_layer = nn.Sequential(\n",
    "#             nn.Linear(8, 16),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(16, 16),\n",
    "#             nn.LeakyReLU(),\n",
    "#         )\n",
    "        \n",
    "#         self.imu_rnn = nn.LSTM(input_size=16, hidden_size=16, num_layers=2, batch_first=True)\n",
    "        \n",
    "#         self.imu_last = nn.Sequential(\n",
    "#             nn.Linear(16, 16),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(16, num_of_classes),\n",
    "#             nn.Softmax(dim=1),\n",
    "#         )\n",
    "        \n",
    "        ################\n",
    "        ### Mag Part ###\n",
    "        ################\n",
    "        self.mag_layer = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.mag_transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=64, nhead=8, dropout=0.1, batch_first=True), num_layers=4)\n",
    "        self.mag_transformer_decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model=64, nhead=8, dropout=0.1, batch_first=True), num_layers=4)\n",
    "        self.mag_last = nn.Sequential(\n",
    "            nn.Linear(64, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 3),\n",
    "        )\n",
    "        \n",
    "    def forward(self, source_mag, target_mag=None):\n",
    "        ### imu part ###\n",
    "        if target_mag != None:\n",
    "\n",
    "            ### mag part ###\n",
    "            source_mag_h = self.mag_layer(source_mag)  # (batch, seq_len, 16)\n",
    "            source_mag_h = self.pos_encoder(source_mag_h)\n",
    "            target_mag = torch.concat([torch.zeros(len(target_mag), 1,  3).to(source_mag.device), target_mag], dim=1)\n",
    "            tgt = self.mag_layer(target_mag)\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "            source_mag_hz= self.mag_transformer_encoder(source_mag_h)  # (batch, seq_len, 16)\n",
    "\n",
    "#             source_latent = torch.add(source_hz, source_mag_hz)  # (batch, seq_len, 48)\n",
    "            tgt_mask = get_tgt_mask()\n",
    "            predict_mag_latent = self.mag_transformer_decoder(tgt, source_mag_hz)\n",
    "            predict_mag = self.mag_last(predict_mag_latent)\n",
    "\n",
    "            return predict_mag\n",
    "        \n",
    "        else:\n",
    "            \n",
    "#             source_h = self.imu_layer(source_imu)\n",
    "#             source_hz, _ = self.imu_rnn(source_h)\n",
    "#             source_predict_probability = self.imu_last(source_hz[:, -1])\n",
    "\n",
    "            ### mag part ###\n",
    "            source_mag_h = self.mag_layer(source_mag)  # (batch, seq_len, 16)\n",
    "            source_mag_h = self.pos_encoder(source_mag_h)\n",
    "            source_mag_hz= self.mag_transformer_encoder(source_mag_h)  # (batch, seq_len, 16)\n",
    "\n",
    "#             source_latent = torch.add(source_hz, source_mag_hz)  # (batch, seq_len, 48)\n",
    "            tgt = torch.zeros_like(source_mag_hz)\n",
    "            for i in range(source_mag_hz.size(1)):\n",
    "                tgt_pos = self.pos_encoder(torch.zeros(source_mag_hz.size(0), 1, source_mag_hz.size(-1)).to(source_mag_hz.device) + i)\n",
    "                decode_position = self.mag_transformer_decoder(tgt_pos, source_mag_hz[:, :i+1, :], memory_key_padding_mask=None)\n",
    "                tgt[:, i, :] = decode_position[:, -1, :]\n",
    "            predict_mag = self.mag_last(tgt)\n",
    "            return predict_mag\n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :].to(x.device)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9366367b-10db-4dd5-972b-eb89ad061ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "front_pocket_half = int(len(front_pocket_pair_data) / 2)\n",
    "pocket_half = int(len(pocket_pair_data) / 2)\n",
    "swing_half = int(len(swing_pair_data) / 2)\n",
    "\n",
    "train_data = front_pocket_pair_data[:front_pocket_half] #+ pocket_pair_data[:pocket_half] + swing_pair_data[:swing_half]\n",
    "valid_data = front_pocket_pair_data[front_pocket_half:] #+ pocket_pair_data[pocket_half:] + swing_pair_data[swing_half:]\n",
    "\n",
    "# train\n",
    "train_source_data = np.array([d[0] for d in train_data])\n",
    "train_source_label = np.array([d[1] for d in train_data])\n",
    "train_target_data = np.array([d[2] for d in train_data])\n",
    "train_target_label = np.array([d[3] for d in train_data])\n",
    "train_dataset = PairDataset(\n",
    "                    source_data = torch.tensor(train_source_data, dtype=torch.float),\n",
    "                    source_label = train_source_label,\n",
    "                    target_data = torch.tensor(train_target_data, dtype=torch.float),\n",
    "                    target_label = train_target_label,\n",
    "                )\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# valid\n",
    "valid_source_data = np.array([d[0] for d in valid_data])\n",
    "valid_source_label = np.array([d[1] for d in valid_data])\n",
    "valid_target_data = np.array([d[2] for d in valid_data])\n",
    "valid_target_label = np.array([d[3] for d in valid_data])\n",
    "valid_dataset = PairDataset(\n",
    "                    source_data = torch.tensor(valid_source_data, dtype=torch.float),\n",
    "                    source_label = valid_source_label,\n",
    "                    target_data = torch.tensor(valid_target_data, dtype=torch.float),\n",
    "                    target_label = valid_target_label,\n",
    "                )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b88b083b-cc71-47d3-8491-780712b263eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72fc51ec-0355-4e62-8804-94684bfb87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1674672d-763d-47dc-a44c-7e7820893068",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000\n",
    "num_of_classes = 4\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31e23ee4-8da1-4fc7-bbd9-0b2397de5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NotSimpleTransformer(seq_len=int(datapoint_per_second * duration), num_of_classes=num_of_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "mse_loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eba50865-9a85-43cb-8719-61f8868194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    pred_loss = []\n",
    "\n",
    "    for source_data, source_label, target_data, target_label in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        source_data = source_data.to(device)\n",
    "        target_data = target_data.to(device)\n",
    "\n",
    "        one_hot = F.one_hot(source_label, num_classes=num_of_classes).to(device).float()\n",
    "\n",
    "        predict_mag = model(source_data[:, :, 6:9], target_data[:, :-1, 6:9])\n",
    "        torch.cuda.empty_cache()\n",
    "        #_, predict_classes = torch.max(predict_probability, 1)\n",
    "\n",
    "#         class_loss = ce_loss(predict_probability, one_hot)\n",
    "        predict_loss = mse_loss(predict_mag, target_data[:, :, 9:12])\n",
    "        loss = predict_loss\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "#         class_loss.backward()\n",
    "#         predict_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        pred_loss.append(predict_loss.item())\n",
    "    \n",
    "    return np.mean(losses), np.mean(pred_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cb16192-e16c-4d77-bc8f-7d1b6eff1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    pred_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for source_data, source_label, target_data, target_label in dataloader:\n",
    "            source_data = source_data.to(device)\n",
    "            target_data = target_data.to(device)\n",
    "            \n",
    "            one_hot = F.one_hot(source_label, num_classes=num_of_classes).to(device).float()\n",
    "\n",
    "            #############\n",
    "            # generator #\n",
    "            #############\n",
    "            predict_mag = model(source_data[:, :, 9:12])\n",
    "            #_, predict_classes = torch.max(predict_probability, 1)\n",
    "\n",
    "            #class_loss = ce_loss(predict_probability, one_hot)\n",
    "            predict_loss = mse_loss(predict_mag, target_data[:, :, 9:12])\n",
    "            loss = predict_loss\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            pred_loss.append(predict_loss.item())\n",
    "    \n",
    "    return np.mean(losses), np.mean(pred_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5e74f03-1f6e-4245-976b-7ef42b7c366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_tgt_mask() missing 1 required positional argument: 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCH):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#####\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# 1. 用上半部訓練50epoch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# 2. 隨機用上或下半部訓練Model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# 3. 印出trajectory結果\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#####\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     train_loss, train_pred_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     valid_loss, valid_pred_loss \u001b[38;5;241m=\u001b[39m evalute(model, valid_loader)\n\u001b[1;32m     11\u001b[0m     ep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m         target_data \u001b[38;5;241m=\u001b[39m target_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m         one_hot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(source_label, num_classes\u001b[38;5;241m=\u001b[39mnum_of_classes)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 15\u001b[0m         predict_mag \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m#_, predict_classes = torch.max(predict_probability, 1)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         class_loss = ce_loss(predict_probability, one_hot)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mag39/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 57\u001b[0m, in \u001b[0;36mNotSimpleTransformer.forward\u001b[0;34m(self, source_mag, target_mag)\u001b[0m\n\u001b[1;32m     54\u001b[0m             source_mag_hz\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmag_transformer_encoder(source_mag_h)  \u001b[38;5;66;03m# (batch, seq_len, 16)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#             source_latent = torch.add(source_hz, source_mag_hz)  # (batch, seq_len, 48)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m             tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[43mget_tgt_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m             predict_mag_latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmag_transformer_decoder(tgt, source_mag_hz)\n\u001b[1;32m     59\u001b[0m             predict_mag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmag_last(predict_mag_latent)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_tgt_mask() missing 1 required positional argument: 'size'"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    #####\n",
    "    # 1. 用上半部訓練50epoch\n",
    "    # 2. 隨機用上或下半部訓練Model\n",
    "    # 3. 印出trajectory結果\n",
    "    #####\n",
    "    \n",
    "    train_loss, train_pred_loss = train(model, train_loader, optimizer)\n",
    "    valid_loss, valid_pred_loss = evalute(model, valid_loader)\n",
    "    \n",
    "    ep = str(epoch).zfill(5)\n",
    "\n",
    "    print(f'{ep}: train total loss: {train_loss: 2.3f}, pred loss: {train_pred_loss: 2.3f}, valid total loss: {valid_loss: 2.3f}, pred loss: {valid_pred_loss: 2.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23719eaf-64e5-4011-b6e3-99c822ddf8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_eval(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for source_data, source_label, target_data, target_label in dataloader:\n",
    "            source_data = source_data.to(device)\n",
    "            target_data = target_data.to(device)\n",
    "            \n",
    "            one_hot = F.one_hot(source_label, num_classes=num_of_classes).to(device).float()\n",
    "\n",
    "            #############\n",
    "            # generator #\n",
    "            #############\n",
    "            predict_mag = model(source_data[:, :, 9:12])\n",
    "            #_, predict_classes = torch.max(predict_probability, 1)\n",
    "\n",
    "            #class_loss = ce_loss(predict_probability, one_hot)\n",
    "            predict_loss = mse_loss(predict_mag, target_data[:, :, 9:12])\n",
    "            loss = predict_loss\n",
    "            \n",
    "            #print(f'{i: >3} predict class: {predict_classes.cpu().detach().numpy()}')\n",
    "            #print(f'{\"\": >3}  ground truth: {labels.numpy()}')\n",
    "\n",
    "            #loss = bce_loss(predict_probability, one_hot)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            #accuracies.append(accuracy_score(labels.cpu().detach().numpy(), predict_classes.cpu().detach().numpy()))\n",
    "            \n",
    "    print(f'loss: {np.mean(losses): 2.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c0beb-8280-4a77-a23a-e421405bb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_eval(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1bea9-59c8-44c6-9423-078f3cefe4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fab0c5-1510-4d9b-b7d9-349b2d245767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mag39",
   "language": "python",
   "name": "mag39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
